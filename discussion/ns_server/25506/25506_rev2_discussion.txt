======================================================================================================================
DESCRIPTION:

MB-100: slashed CPU consumption of timer

I've found stdlib's timer to burn CPU without good reason. Here's what
happens.

The problem is that it sleeps in milliseconds but computes time in
microseconds. So there's simple and I'd say somewhat funny bug in code
to compute milliseconds to sleep. They compute microseconds difference
between now and nearest timer event and then do _truncating_ division
by 1000. So on average they sleep 500 microseconds _less than
needed_. On wakeup they check do I have timer tick that already
occurred? No. Ok how much I need to sleep ? They do that bad
computation again and get 0 milliseconds. So next gen_server timeout
happens right away only to find we're still before closest timer tick
and to decide to sleep 0 milliseconds again. And again and again.

This change adds fork of timer that keeps track of time in
milliseconds. The idea is given all intervals and sleep durations are
in milliseconds anyways, there's no benefit at all keeping tack of
time in microseconds.

Rest of change replaces all occurences of timer:xxx_interval and
timer:xxx_after with same calls to timer2 module.

Change-Id: I469a136001fb314559a733acf0a80b598a74c194

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Aliaksey Kandratsenka
Date: 2013-04-06 01:27:12.000000000
Message: 
Uploaded patch set 2.
----------------------------------------------------------------------------------------------------------------------
