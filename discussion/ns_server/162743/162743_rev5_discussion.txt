======================================================================================================================
DESCRIPTION:

MB-46215 Collect event logs in /diag

1) Collect the event_log file as a part of cbcollect_info.
2) Dump all the event logs via /diag endpoint.

Change-Id: Icd91075331cc18ca6a4cba7a661a539eb9534aea

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Hareen Kancharla
Date: 2021-10-04 19:39:44.000000000
Message: 
Uploaded patch set 5: Patch Set 4 was rebased.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2021-10-04 19:40:14.000000000
Message: 
Patch Set 5: -Well-Formed

Build Started http://cv.jenkins.couchbase.com/job/ns-server-test/26688/
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2021-10-04 19:45:27.000000000
Message: 
Patch Set 5: Well-Formed+1

Build Successful 

http://cv.jenkins.couchbase.com/job/ns-server-test/26688/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Hareen Kancharla
Date: 2021-10-04 19:51:04.000000000
Message: 
Patch Set 5: Verified+1
----------------------------------------------------------------------------------------------------------------------
Author: Abhijeeth Nuthan
Date: 2021-10-04 23:01:49.000000000
Message: 
Patch Set 5:

(1 comment)
Line:494, src/diag_handler.erl -> This seems pointless since we collect the file anyway.
What is the reason to put this much memory and cpu pressure when we are collecting logs?

----------------------------------------------------------------------------------------------------------------------
Author: Hareen Kancharla
Date: 2021-10-04 23:38:19.000000000
Message: 
Patch Set 5:

(1 comment)
Line:494, src/diag_handler.erl -> Because the one on disk (event_log) is a list of erlang terms comprising all the event logs and are not JSON logs. They look like this.

[[{seq_num,1}|
   {log_entry,"2021-09-29T18:35:23.036Z",
    <<"c1b3e4e768a7226b787af6ce84ff9c33">>,
    [{timestamp,<<"2021-09-29T18:35:23.036Z">>},
     {uuid,<<"c1b3e4e768a7226b787af6ce84ff9c33">>},
     {event_id,9222},
     {component,security},
     {description,<<"Password policy changed">>},
     {severity,info},
     {node,<<"127.0.0.1">>},
     {extra_attributes,
      {struct,
       [{sub_component,menelaus_web_rbac},
        {old_settings,{struct,[{min_length,6},{must_present,[]}]}},
        {new_settings,
         {struct,[{min_length,8},{must_present,[uppercase]}]}}]}}]}]]%

Services wanted us to collect the logs as JSON in cbcollect if they'll need it for debugging.

----------------------------------------------------------------------------------------------------------------------
Author: Meni Hillel
Date: 2021-10-04 23:51:31.000000000
Message: 
Patch Set 5:

(1 comment)
Line:494, src/diag_handler.erl -> The current format of these files are not readable and support had asked for it in a json format.

----------------------------------------------------------------------------------------------------------------------
Author: Hareen Kancharla
Date: 2021-10-06 19:36:59.000000000
MISMATCHED INLINE COMMENT
Line:494, src/diag_handler.erl -> It helps to have them as JSON somewhere if somebody needs them to debug. It doesn't help us to do it in the diag_handler for the reasons you mentioned. I am thinking of a offline erlang script which can uncompress event_log and convert the erlang terms to JSON similar to what we have in 

@Meni: Is that ok? Services should be probably ok to run the script if they need the internal event_log file  (collected via cbcollectinfo) as a JSON? 

I removed the diag_handler bit for now and will push a separate patch for that.
----------------------------------------------------------------------------------------------------------------------
Author: Abhijeeth Nuthan
Date: 2021-10-06 17:08:43.000000000
MISMATCHED INLINE COMMENT
Line:494, src/diag_handler.erl -> My 2 cents, and I may be the only one to care for this. 

We should have this information when node is not up too. So in this case they are ok not having a json.

We are storing the file in compressed erlang terms because we expect we are the only one who will be be parsing them. If we are not the only ones why not store them as json? Are we concerned about the disk footprint of event logs? 

Alternatively, why can't we provide a script that converts this file to json if required. This need not be run in cbcollect. Support and services only need to care about the format if they are running scripts against this log. Otherwise I would still say is human readable to an extent if uncompressed. Such a script is useful for ns_log too.

Just to get a file in pretty format we shouldn't be putting the system under pressure. We have already enough problems of diag timing out, when system is under load. I would rather not add to this problem. 

event log has much more entries(6 times as much) than ns_log. I would say it's not worth it to extract the event log from a live server especially if we can get the information off disk directly.

> [[{seq_num,1} | {log_entry,

On another note we should change add_local_metadata this doesn't seem right. Should be,
"[{seq_num,1}, {log_entry,"
----------------------------------------------------------------------------------------------------------------------
