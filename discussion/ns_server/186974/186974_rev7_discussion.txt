======================================================================================================================
DESCRIPTION:

MB-55589: Report eunit error as failed test

There exists a bug in the t module responsible for running eunit tests
that causes some eunit errors to be ignored and for tests to appear to
be successful when they have not in fact run at all. Errors during test
group setup (and probably teardown) and errors with the test descriptors
themselves may be ignored. This can result in tests being committed that
don't actually work as they will pass CV, or tests that can be
inadvertently broken if errors are made in the setup functions or with
the test descriptors.

When the t module runs eunit tests it spawns a listener process that
listens to test events generated by eunit. The listener process listens
for a "progress" messages for when a "test" begins and then either an
end "progress" message or a "cancel" message. The end "progress" message
has a status in it which is used to determine whether or not the test
failed. Cancelled "test"s are failed. The listener process aggregates
all of the test failures then reports them at the end to the main
process. The main process uses this list of test failures to determine
the overall result of ns_test (empty list = pass). The main purpose of
this I believe is to report all of the unit test failures at the end of
the ns_test run in one place.

The issue here is that the listener process too aggresively matches the
messages that it is receiving. It only matches messages for "test"
messages which come from eunit running foo_test() test functions. When
eunit runs a test generator fun, bar_test_(), it sends "group" messages
rather than "test" messages. These messages are ignored by the listener
process and so no tests are added to the FailedTests list, and no error
is reported by ns_test.

Whilst we could listen for specific "group" messages to report test
failures for those functions, this solution is not ideal for a few
reasons. Firstly, the "group" messages reported by eunit contain the
name of the module and function that encountered the error, rather than
the name of the test being run. That could prove confusing if setup
functions are re-used in multiple tests. Secondly, the format of these
"group" messages is not always the same. Errors for bad test descriptors
differ to errors from setup functions failing and we would have to match
with multiple types of messages to attempt to extra the pertinent
information. Thirdly, the listener process must report failed tests as a
tuple of {Module, Function, Arity}. For test setup errors we can parse
the message appropriately, but for bad test descriptor errors only the
module is included and it's part of a function object. I've only test
this with setup failure and bad test descriptor errors. Other errors may
have other formats and include different information. This makes this
solution rather brittle.

Rather than listen for "group" messages, we could simply report the
status of the eunit:test function if it was nto successful. This is less
meaningful that reporting that Module:Function/Arity failed, but eunit
will log the pertinent information when it encounters the error. This
should generally be fine when running ns_test for a single module, but
is potentially problematic when running ns_test for all modules. In such
a case it may be hard to impossible to find the error message. As we
plan to split ns_test invocations up into ctest suites for CV, we would
only have to deal with the output for one module so this should be more
reasonable. Locally, the issue would exist, but CV should generally
guard from test failures being merged. We could match some messages in
the listener process to provide output for some commonly hit errors if
not logging the location of the error at the end of the test run proves
problematic. Whilst this approach does have drawbacks, it does allow us
to relatively quickly ensure that these errors are reported correctly,
and it does capture all cases in which eunit encounters an error.

It may be possible to more easily extract this information from eunit.
The reporting feature seems to be practically undocumented, but the
source code looks to be relatively readable. We could experiment a bit
more with the reporting and look for either some common format for
messages or less brittle way to track failed tests so that we can report
them at the end.

Bad test descriptor error before:

Running eunit tests for modules: [bucket_info_cache]
======================== EUnit ========================
undefined
*** bad test descriptor ***
**{setupx,#Fun<bucket_info_cache.24.6118393>,
          #Fun<bucket_info_cache.25.6118393>,
          [{{[7,1],false,false},#Fun<bucket_info_cache.23.6118393>},
...
           {{[7,5],true,true},#Fun<bucket_info_cache.23.6118393>}]}

=======================================================
  Failed: 0.  Skipped: 0.  Passed: 0.
One or more tests were cancelled.
Running triq tests for modules: [bucket_info_cache]

Bad test descriptor error after:

Running eunit tests for modules: [bucket_info_cache]
======================== EUnit ========================
undefined
*** bad test descriptor ***
**{setupx,#Fun<bucket_info_cache.24.6118393>,
          #Fun<bucket_info_cache.25.6118393>,
          [{{[7,1],false,false},#Fun<bucket_info_cache.23.6118393>},
...
           {{[7,5],true,true},#Fun<bucket_info_cache.23.6118393>}]}

=======================================================
  Failed: 0.  Skipped: 0.  Passed: 0.
One or more tests were cancelled.
Running triq tests for modules: [bucket_info_cache]
=======================================================
  Failed tests:
    eunit:error/0
=======================================================
CMake Error at cmake_modules/do-test.cmake:47 (MESSAGE):
  failed running tests

Failed setup error before:

Running eunit tests for modules: [bucket_info_cache]
======================== EUnit ========================
module 'bucket_info_cache'
  undefined
  *** context setup failed ***
**in function erlang:throw/0
  called as throw()
in call from bucket_info_cache:membase_bucket_capabilities_test_setup/3 (src/bucket_info_cache.erl, line 517)
**error:undef

undefined
*** context setup failed ***
**in function meck_proc:start/2 (src/meck_proc.erl, line 97)
  called as start(cluster_compat_mode,[passthrough])
in call from bucket_info_cache:setup_compat_mode_for/2 (src/bucket_info_cache.erl, line 488)
in call from bucket_info_cache:membase_bucket_capabilities_test_setup/3 (src/bucket_info_cache.erl, line 516)
**error:{already_started,<0.131.0>}

[done in 0.292 s]
=======================================================
  Failed: 0.  Skipped: 0.  Passed: 0.
One or more tests were cancelled.
Running triq tests for modules: [bucket_info_cache]

Failed setup error after:

Running eunit tests for modules: [bucket_info_cache]
======================== EUnit ========================
module 'bucket_info_cache'
  undefined
  *** context setup failed ***
**in function erlang:throw/0
  called as throw()
in call from bucket_info_cache:membase_bucket_capabilities_test_setup/3 (src/bucket_info_cache.erl, line 517)
**error:undef

undefined
*** context setup failed ***
**in function meck_proc:start/2 (src/meck_proc.erl, line 97)
  called as start(cluster_compat_mode,[passthrough])
in call from bucket_info_cache:setup_compat_mode_for/2 (src/bucket_info_cache.erl, line 488)
in call from bucket_info_cache:membase_bucket_capabilities_test_setup/3 (src/bucket_info_cache.erl, line 516)
**error:{already_started,<0.131.0>}

[done in 0.324 s]
=======================================================
  Failed: 0.  Skipped: 0.  Passed: 0.
One or more tests were cancelled.
Running triq tests for modules: [bucket_info_cache]
=======================================================
  Failed tests:
    eunit:error/0
=======================================================
CMake Error at cmake_modules/do-test.cmake:47 (MESSAGE):
  failed running tests

Change-Id: I6af54c01dafbb70612ae185e589e2b8363502239

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Ben Huddleston
Date: 2023-02-24 18:01:31.000000000
Message: 
Patch Set 7: Patch Set 6 was rebased
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2023-02-24 18:01:39.000000000
Message: 
Patch Set 7: -Well-Formed

Build Started https://cv.jenkins.couchbase.com/job/ns-server-test/34728/ (1/2)
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2023-02-24 18:02:41.000000000
Message: 
Patch Set 7:

Build Started https://cv.jenkins.couchbase.com/job/ns-server-ns-test/2160/ (2/2)
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2023-02-24 18:05:39.000000000
Message: 
Removed Code-Review-1 by <GERRIT_ACCOUNT_1004341>

----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2023-02-24 18:21:47.000000000
Message: 
Patch Set 7: Well-Formed+1

Build Successful 

https://cv.jenkins.couchbase.com/job/ns-server-test/34728/ : SUCCESS

https://cv.jenkins.couchbase.com/job/ns-server-ns-test/2160/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2023-02-24 18:23:35.000000000
Message: 
Patch Set 7: Verified+1
----------------------------------------------------------------------------------------------------------------------
Author: Hareen Kancharla
Date: 2023-02-24 18:59:32.000000000
Message: 
Patch Set 7:

(1 comment)
Line:111, test/t.erl -> Minor suggestion: TestResult could potentially be a tuple too and format_mfa/1 would throw an exception. Could change the format specifier for the Function arg to ~p.

----------------------------------------------------------------------------------------------------------------------
Author: Timofey Barmin
Date: 2023-02-24 19:22:04.000000000
Message: 
Patch Set 7:

(1 comment)
Line:108, test/t.erl -> I suggest we report better error description in case if some test setup fails.
Currently I see:

  [31;1mFailed tests[0m:
    eunit:error/0
    
which IMHO is confusing.

----------------------------------------------------------------------------------------------------------------------
Author: Hareen Kancharla
Date: 2023-02-24 19:27:30.000000000
Message: 
Patch Set 7:

(1 comment)
Line:108, test/t.erl -> Second that.

----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2023-03-03 15:09:05.000000000
MISMATCHED INLINE COMMENT
Line:108, test/t.erl -> Done
----------------------------------------------------------------------------------------------------------------------
Author: Timofey Barmin
Date: 2023-03-02 18:37:08.000000000
MISMATCHED INLINE COMMENT
Line:108, test/t.erl -> It looks like you can put whatever you like in the list of failed test here, and then just right a special handler for it in format_mfa(). If it is hard to output something specific, you at least can print something more generic using English language and try explaining where to look for error.
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2023-03-02 08:58:50.000000000
MISMATCHED INLINE COMMENT
Line:108, test/t.erl -> I agree, and I would like to do something better, but as mentioned in the commit message that isn't super simple. Eunit has slightly different formats for all of these failure messages so it's hard to write something that catches setup failures, bad test descriptors, and maybe teardown failures too (but I haven't tested that). Those messages don't map easily to the Module:Function/Arity that we currently log, so we'd also have to restructure the test output too to provide some meaningful error. None of it is too difficult, but it isn't particularly high in the priority list at the moment. Ideally I'd like to merge this change now as is to prevent unit tests from being commits that do not work and improve the logging in a followup.

FWIW I still plan on creating a ctest suite per module (with unit tests) which makes the test output far more readable.
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2023-03-02 08:58:50.000000000
MISMATCHED INLINE COMMENT
Line:111, test/t.erl -> Done
----------------------------------------------------------------------------------------------------------------------
