======================================================================================================================
DESCRIPTION:

MB-40724: Limit the depth of NSLog when formatting in dump-guts

... otherwise large log entries consume gigabytes of RAM during
formatting

Change-Id: I6ef08827e7590bccfa6f911419beb0625d139021

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Timofey Barmin
Date: 2020-09-14 22:08:19.000000000
Message: 
Uploaded patch set 1.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2020-09-14 22:08:29.000000000
Message: 
Patch Set 1: Well-Formed-1

Branch restricted. PLEASE READ this URL: 

http://server.jenkins.couchbase.com/job/restricted-branch-check/200636/artifact/restricted.html : FAILURE
----------------------------------------------------------------------------------------------------------------------
Author: Timofey Barmin
Date: 2020-09-14 22:08:41.000000000
Message: 
Patch Set 1: Verified+1
----------------------------------------------------------------------------------------------------------------------
Author: Steve Watanabe
Date: 2020-09-14 22:32:19.000000000
Message: 
Patch Set 1:

(1 comment)
Line:627, scripts/dump-guts -> 100000 seems pretty large. How deep are the items being formatted? How much does the memory usage go down?

----------------------------------------------------------------------------------------------------------------------
Author: Dave Finlay
Date: 2020-09-14 22:58:33.000000000
Message: 
Patch Set 1: Code-Review+2

(1 comment)
Line:627, scripts/dump-guts -> I spoke with Timofey over slack and asked the same question. Basically the memory spike disappears entirely. 

There's another issue which is that the NSLog is a list and the first item in the list gets a depth of 100,000 but the last time in the list gets a depth of 97,000 (since the max size of the list is 3k.) This seems odd - we should treat each log_entry equally, but Timofey's idea here is that 100,000 is essentially never hit except for args to the log_entry which are pathological. 100,000 and 97,000 are equally effective very large bounds that have no effect whatsoever on normal log entries, while dealing effectively with pathological log entries. 

@Timofey: It would be good to add this last idea as a comment (in some concise way) in the code so that folks come later know what you're doing with this 100,000 number.

----------------------------------------------------------------------------------------------------------------------
Author: Dave Finlay
Date: 2020-09-14 23:00:24.000000000
Message: 
Patch Set 1:

check approval
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2020-09-14 23:00:36.000000000
Message: 
Patch Set 1: Well-Formed+1

Permission granted to commit: 

http://server.jenkins.couchbase.com/job/restricted-branch-check/200641/artifact/restricted.html : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Timofey Barmin
Date: 2020-09-14 23:56:05.000000000
MISMATCHED INLINE COMMENT
Line:627, scripts/dump-guts -> Done. Not sure about conciseness, but I believe it should work.
----------------------------------------------------------------------------------------------------------------------
