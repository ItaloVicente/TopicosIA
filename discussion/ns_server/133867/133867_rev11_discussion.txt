======================================================================================================================
DESCRIPTION:

MB-40711 Trim gathered stats if needed by size

If the size of a prometheus snapshot exceeds the specified limit we'll
delete "blocks" until we're within the limit (or there's only one left).

Change-Id: I50b42f39d9fe5c03e24eef43d5f8dccbc0892a8b

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Steve Watanabe
Date: 2020-09-08 20:08:08.000000000
Message: 
Uploaded patch set 11.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2020-09-08 20:08:21.000000000
Message: 
Patch Set 11:

Build Started http://cv.jenkins.couchbase.com/job/ns-server-test/18808/
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2020-09-08 20:12:52.000000000
Message: 
Patch Set 11: Well-Formed+1

Permission granted to commit to restricted branch
----------------------------------------------------------------------------------------------------------------------
Author: Steve Watanabe
Date: 2020-09-08 20:27:42.000000000
Message: 
Patch Set 11:

> Patch Set 10:
> 
> This algorithm will lead to giant cbcollect dumps in most common use-case when there is small amount of "high cardinality" metrics.
> In other words, say max size is 1GB, we will always have 1GB cbcollect dumps if cluster runs long enough. What we actually want to achieve is to have 1GB dump only in case if we have abnormal number of metrics (when we can't avoid it), and normal (rather small) dump size in other (most common) cases.
> Please see my last comment about this in "Stats & cbcollect in Cheshire-Cat" document.

Dave has an idea to decimate the stats...to do what is currently done...increase the granularity size for older stats.  e.g. stats every 10 seconds for six? hours, then stats every minute for a day, then stats every hour for a week, ...I'm just picking values here as an example.  The idea would be to periodically use the prometheus delete series operation in a "smart" manner to do this. The details need to be worked out.
----------------------------------------------------------------------------------------------------------------------
