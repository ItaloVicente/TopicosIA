======================================================================================================================
DESCRIPTION:

implementation of quorum loss chronicle failover

In case of unsafe failover the nodes are failed over in chronicle
first, then the regular failover routine is done and at the end
the nodes will be removed (not deactivated) so they cannot be
added back.

In case of unfinished failover, the key $failover_opaque will
contain the list of nodes that were unsuccessfully failed over.
The next failover will have to include these nodes, so the
failover should be properly finished.

Change-Id: Ib63b80c9556b659ab32793ba0c37a3126b5cfe41

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Artem Stemkovski
Date: 2021-03-09 19:59:47.000000000
Message: 
Uploaded patch set 10: Commit message was updated.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2021-03-09 20:00:06.000000000
Message: 
Patch Set 10:

Build Started http://cv.jenkins.couchbase.com/job/ns-server-test/22367/
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2021-03-09 20:04:31.000000000
Message: 
Patch Set 10: Well-Formed+1

Permission granted to commit to restricted branch
----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-03-10 21:52:17.000000000
Message: 
Patch Set 10:

(1 comment)
Line:136, src/failover.erl -> This looks like a bug. We shouldn't change membership statuses for nodes that still have active vbuckets. Because then they can be ejected from the cluster while vbucket maps will continue to refer to them.

----------------------------------------------------------------------------------------------------------------------
Author: Artem Stemkovski
Date: 2021-03-11 00:04:39.000000000
Message: 
Patch Set 10:

(1 comment)
Line:136, src/failover.erl -> Here's where it comes from:
http://review.couchbase.org/c/ns_server/+/88661/22/src/ns_rebalancer.erl

Should we increase the counter and then crash?

----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-03-11 01:04:01.000000000
Message: 
Patch Set 10:

(1 comment)
Line:136, src/failover.erl -> At the time only service failover could fail. And the service map would be modified even if the actual failover didn't succeed. So it was probably ok to do at the time (assuming services handled this case correctly). It's different now that bucket failover may also fail. If any of the vbucket maps still refer to some node, it may not be marked inactiveFailed.

So to answer to your question: yes, we should return an error and not deactivate nodes.

----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-03-11 01:12:44.000000000
Message: 
Patch Set 10:

(1 comment)
Line:136, src/failover.erl -> Thinking about it more. Do I understand correctly that we never returned any errors about failover failing in the middle (even though that was the intent)? Or am I missing something?

----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-03-11 01:23:56.000000000
Message: 
Patch Set 10:

(1 comment)
Line:136, src/failover.erl -> We should also probably fix it in 6.6.*

----------------------------------------------------------------------------------------------------------------------
Author: Artem Stemkovski
Date: 2021-03-11 01:25:49.000000000
Message: 
Patch Set 10:

(1 comment)
Line:136, src/failover.erl -> Well, the user experience now is that failover doesn't change anything and there's an error in error log.

----------------------------------------------------------------------------------------------------------------------
Author: Artem Stemkovski
Date: 2021-03-11 05:31:18.000000000
MISMATCHED INLINE COMMENT
Line:136, src/failover.erl -> Disabling not properly failed over nodes is obviously an old bug, but how do you suggest to report errors other than writing them to user log?
----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-03-11 16:51:07.000000000
MISMATCHED INLINE COMMENT
Line:136, src/failover.erl -> How do we report rebalance errors?
----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-03-11 03:48:22.000000000
MISMATCHED INLINE COMMENT
Line:136, src/failover.erl -> The user experience is (if I read the code correctly).

1. Failover fails half-way through.
2. User doesn't get any indication that anything went wrong.
3. Nodes are marked as failed over. Even though they are not.
4. If the user realizes that failover didn't go through, there's no way for them to retry.
5. Worse yet, if they eject failed over nodes, the cluster will be in an inconsistent state.
----------------------------------------------------------------------------------------------------------------------
