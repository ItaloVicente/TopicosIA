======================================================================================================================
DESCRIPTION:

MB-48930: Remove user tombstones hourly

Creates an agent on all nodes that waits for requests from a single
"manager" on the leader that coordinates each node to remove the
tombstones. This is done in basically this process:

 1. Agent on all nodes, manager on the leader
 2. Every hour the manager wakes up and orchestrates a multi-phased
    job to remove tombstones.
 3. Manager sends prepare message to notify all nodes that we are
    beginning the process. On agent recv of this message, user
    requests are paused until the process completes. This is managed
    by the process, or more effectively an ETS table that mirrors it's
    information.
 4. All agents respond with 'ok' when ready.
 5. The manager requests the set of all keys for all tombstones on
    each particular node, from the agents.
 6. The intersection of the set of all the keys is determined on the
    manager.
 7. That final set of keys, on which all nodes agree, is sent back to
    all the agents to be removed.
 8. The agents take this set of keys and constructs a select_delete to
    remove all these keys, that are still active tombstones. These are
    removed from the DETS and ETS table in memory.
 9. The DETS changes are synced to disk, the file is closed, and then
    reopened to compact the *.dets file. This can only happen as it's
    opened due to whatever limitations there are in the erlang API for
    this functionality. Otherwise this file will grow indefinitely
    until it hits the 2gb max.
 10. Responses are sent back to manager and user requests are allowed
     to continue.

This is the basic flow of a happy-path execution and there are a
number of fail-safes built into the process, but luckily most of what
we are doing is entirely reversable at most points of failure, and
will be reversed automatically by virtue of the replication algorithm.
This means if we only delete the tombstones on 2 of 3 nodes, we don't
have to figure out a way to undo the operations, but instead the
replication algorithm will simple re-replicate the tombstones back to
the successful nodes and we are at square one.

* Design:

  The basic design is relatively simple and involves agents on all
  nodes, and a manager on the leader. The agents are made up of a
  state machine (gen_statem) that simplifies this multi-phase process
  and gives a number of tools to deal with the possible failure
  scenarios such as state timeouts, transitions, and a general
  structure that makes assessing behavior easier. We also have the
  agents place a monitor on the manager process while they are in the
  process to detect for the manager crashing, though this would also
  be safeguarded by the state timeouts, which you can say how long you
  are allowed to be in a state before an event fires to notify you
  that this time has elapsed. The manager is just a gen_server that
  goes through a number of multi-request/responses in succession until
  the phases of the process have completed and to handle and detect
  errors along the way.

* Other notes:

  - We don't even actually start the manager if not all nodes are above
    a certain version. The agents will start, but simply sit in
    waiting, with no requests being stopped by the menelaus_web
    handlers.

* Correctness:
  - We can determine the correctness of this algorithm by thinking of
    each node as a weakly consistent set (which is what it is) and
    consider some cases.

    Case 1: All sets are the same, and no messages are mid-flight.
         This is obviously the simple case where it should be obvious
         that it is correct for us to remove the tombstones in this
         case.
    Case 2: All sets are the same, but there are more deletes that get
         processed by one of the nodes between getting the keys, and
         purging. Because we will only attempt to purge the keys we
         knew about at the time, even if there are others that haven't
         made their way through replication, it will not result in
         incorrect, just (potentially) incomplete purging.
    Case 3: In flight update resurrecting a deleted tombstone on one
         node that other nodes haven't seen yet. In this case, since
         we guard all the select_delete statments with the {deleted,
         true} metadata, this entry will not be selected, despite
         originally being in the list of keys agreed upon. That means
         we won't accidentally clobber this resurrected key.
    Case 4: Any combination of creates/updates/deletes slip by between
         collect keys phase, and purge phase. Creates and updates
         should generally not affect the correctness, unless they
         resurrect tombstones (see case 3). And further deletes, will
         simply not be represented in the cleaned tombstones, to be
         deleted on the next hours purge.

Change-Id: Iab028cc82a2e92e957507bc5a30c3574de0871cd

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Bryan McCoid
Date: 2021-12-22 19:15:56.000000000
Message: 
Uploaded patch set 16.
----------------------------------------------------------------------------------------------------------------------
Author: Restriction Checker
Date: 2021-12-22 19:16:07.000000000
Message: 
Patch Set 16: Well-Formed+1

Permission granted to commit: 

http://server.jenkins.couchbase.com/job/restricted-branch-check/279525/artifact/restricted.html : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Bryan McCoid
Date: 2021-12-22 20:22:04.000000000
Message: 
Patch Set 16:

(4 comments)
----------------------------------------------------------------------------------------------------------------------
Author: Hareen Kancharla
Date: 2021-12-22 22:00:43.000000000
Message: 
Patch Set 16:

(3 comments)
Line:204, src/user_tombstone_agent.erl -> Nit: node_ok/1 seems superfluous - should probably get rid of it not really necessary.

Line:40, src/user_tombstone_manager.erl -> Minor: I think we should move purge to handle_info and then simply call - erlang:send_after(...,...,purge); we seem to follow that pattern in general in the code.

sending {'$gen_cast', purge} assumes knowing that's how the gen_server internally converts those messages into handle calls.

----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-12-22 22:44:21.000000000
Message: 
Patch Set 16:

(1 comment)
----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-12-22 23:03:02.000000000
Message: 
Patch Set 16:

(2 comments)
Line:93, src/user_tombstone_agent.erl -> This is still incorrect. It's possible to have multiple leaders, in which case different nodes will monitor different processes.

Line:138, src/user_tombstone_agent.erl -> I don't think it's safe to abort here. If some nodes complete the purge and some don't, we'll get exposed to all sorts of anomalies.

----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-12-22 23:07:24.000000000
Message: 
Patch Set 16:

I don't see how this patch can be made work. The approach is roughly speaking a two-phase commit. But it's a two phase commit where nodes can randomly abort. That's not how two-phase commit is supposed to work. Once the decision is made to commit, the commit must happen. But it also means that any operations  on users may be unavailable forever/for a very long period of time.
----------------------------------------------------------------------------------------------------------------------
Author: Bryan McCoid
Date: 2021-12-22 23:29:40.000000000
Message: 
Patch Set 16:

(2 comments)

> Patch Set 16:
> 
> I don't see how this patch can be made work. The approach is roughly speaking a two-phase commit. But it's a two phase commit where nodes can randomly abort. That's not how two-phase commit is supposed to work. Once the decision is made to commit, the commit must happen. But it also means that any operations  on users may be unavailable forever/for a very long period of time.

So it's just impossible, in your opinion?
Line:93, src/user_tombstone_agent.erl -> This is entirely optional.. it only gives the process a quicker way to abort if the manager quits. The ultimate way for this process to abort is by the timeouts at every stage.

----------------------------------------------------------------------------------------------------------------------
Author: Bryan McCoid
Date: 2021-12-22 23:33:22.000000000
Message: 
Patch Set 16:

(1 comment)
----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-12-22 23:37:20.000000000
Message: 
Patch Set 16:

> So it's just impossible, in your opinion?

Not with the tools available in 6.6.*. Yes, we could do a proper two-phase commit, but then when things go wrong the consequences are pretty disastrous.

We could do some sort of "disabled by default best effort" approach and ignore the races. But then I personally would want the patch to be much simpler.
----------------------------------------------------------------------------------------------------------------------
Author: Bryan McCoid
Date: 2021-12-22 23:41:43.000000000
Message: 
Patch Set 16:

> Patch Set 16:
> 
> > So it's just impossible, in your opinion?
> 
> Not with the tools available in 6.6.*. Yes, we could do a proper two-phase commit, but then when things go wrong the consequences are pretty disastrous.
> 
> We could do some sort of "disabled by default best effort" approach and ignore the races. But then I personally would want the patch to be much simpler.

It is disabled by default currently as suggested. It has to be enabled by an internal settings flag.
----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2021-12-23 14:49:34.000000000
Message: 
Patch Set 16:

(2 comments)

Couple more issues.

1. Concurrent leaders are not dealt with. It looks like it can lead to some nodes aborting leading to inconsistencies.

2. New nodes getting added at the wrong moment break everything.

3. Some of the nodes getting disconnected during purging break everything.

4. Some of the nodes getting restarted during purging break everything.

5. Users may get resurrected due to delayed replications (an update that happened before purging making it to a node after purging).

My opinion is that this approach is unworkable.
Line:93, src/user_tombstone_agent.erl -> Both of these ways to abort are unsafe. That being said, it's still not good to monitor a potentially totally unrelated process.

Line:186, src/user_tombstone_agent.erl -> In addition to the fact that this breaks abstraction layers between modules, this is also raceful: it's possible that you'll decide to purge something that is not deleted (becomes not deleted the moment after this function returns). While do_purge_and_sync() checks for this, I don't believe this is enough: the end result may be that some deletions do not get replicated to all nodes.

----------------------------------------------------------------------------------------------------------------------
Author: Bryan McCoid
Date: 2021-12-23 18:26:52.000000000
Message: 
Abandoned
----------------------------------------------------------------------------------------------------------------------
