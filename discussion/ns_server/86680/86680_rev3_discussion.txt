======================================================================================================================
DESCRIPTION:

MB-12739: Support for server group auto-failover.

Change-Id: I57bde705ca1690c5fa986098c011ef84c89ac562

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Poonam Dhavale
Date: 2017-12-11 23:22:54.000000000
Message: 
Uploaded patch set 3.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2017-12-11 23:23:03.000000000
Message: 
Patch Set 3:

Build Started http://cv.jenkins.couchbase.com/job/ns-server-test/917/
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2017-12-11 23:33:47.000000000
Message: 
Patch Set 3: Well-Formed+1

Permission granted to commit to restricted branch
----------------------------------------------------------------------------------------------------------------------
Author: Poonam Dhavale
Date: 2017-12-12 21:30:50.000000000
Message: 
Patch Set 3: Verified+1
----------------------------------------------------------------------------------------------------------------------
Author: Artem Stemkovski
Date: 2017-12-15 01:08:36.000000000
Message: 
Patch Set 3:

Is it guaranteed that all replicas of at least one vbucket won't reside in the same server group?
----------------------------------------------------------------------------------------------------------------------
Author: Poonam Dhavale
Date: 2017-12-15 05:10:25.000000000
Message: 
Patch Set 3:

> Is it guaranteed that all replicas of at least one vbucket won't
 > reside in the same server group?


One of the pre-requisites for server group auto-failover is that the groups be balanced with equal number of KV nodes in each group. So, normally, it should not happen that there is a vBucket whose all replicas reside within the same group.
 
However, we do not check for the above condition and it is possible to have scenarios where the groups are unbalanced or become unbalanced and all replicas of some vBuckets lie within the same server group.

But, there is an existing check in auto-failover code to prevent failover of a node if there are unsafe buckets – buckets that have at least one vBucket whose replica cannot be promoted.

If auto-failover of an unbalanced server group is attempted then either failover of some of its nodes may fail due to the unsafe buckets check or if a node is failed over, then its promoted replicas may not come online because they reside on other down nodes. However, this behaviour is no worse than the one in earlier releases or when server group failover is not enabled. Because if the server group failover is not enabled, then none of the down nodes would be failed over anyway.
----------------------------------------------------------------------------------------------------------------------
Author: Aliaksey Artamonau
Date: 2017-12-18 16:21:54.000000000
Message: 
Patch Set 3:

(6 comments)
Line:336, src/auto_failover.erl -> Can this be anything but {failover, {Node, UUID}}? I'd suggest  making that explicit in the code then.

Line:368, src/auto_failover.erl -> I find it counterintuitive that on one hand we say that we're failing over a server group and yet we can failover just a subset of the server group.

On a similar note, I think that all the nodes need to be passed together to the orchestrator to validate the failover and possibly act on it as a whole, not on a node by node basis. Otherwise, it's possible that we'll failover a subset of the subset of the nodes that are down. And we won't get a chance to fix it until the autofailover counter is manually reset by the user.

Line:579, src/auto_failover.erl -> I know that we say that having balanced groups is a prerequisite for the feature. That said, I still think that we need to make an attempt to check that we're not failing over the majority of nodes.

Line:603, src/auto_failover.erl -> Can you please you proplists functions to extract the name and the nodes?

Line:163, src/auto_failover_logic.erl -> Imagine the following situation.

- a node reaches failover state, but auto_failover process fails to act on it (so the node remains in that state).
- then the rest of the nodes in the group go down (maybe that node was the first one to get affected by a fire in the datacenter).

So that first node will forever remain in failover state. The rest of the down nodes will eventually reach failover_group state. So we won't tell auto_failover process to do anything about this.

Also, I'd say that the repeating "case GrpDown of" here is an indication that there's probably a better way to decouple group/node states from each other.

Line:263, src/auto_failover_logic.erl -> It seems possible that should_failover_node returns an empty list.

----------------------------------------------------------------------------------------------------------------------
Author: Poonam Dhavale
Date: 2017-12-27 21:48:16.000000000
MISMATCHED INLINE COMMENT
Line:336, src/auto_failover.erl -> That thought had crossed my mind as well but this code will change for the "auto-failover up to # of replicas” feature. I will be working on it next. So, decided to leave it as is.
----------------------------------------------------------------------------------------------------------------------
Author: Poonam Dhavale
Date: 2017-12-27 21:48:16.000000000
MISMATCHED INLINE COMMENT
Line:368, src/auto_failover.erl -> Assuming customers follow the recommendations suggested for this feature, then in majority of the cases,  auto-failover of a server group should result in failover of all nodes within that group. For other cases, it may be better to failover whichever nodes we can than none of them at all. However, if we get feedback from the field that this is confusing, then we can change the behavior in future release, to failover a server group only when we can failover all of the nodes within that group.

Regarding passing all nodes to the orchestrator for failover for it to act on it as a whole and not on node-by-node basis: Yes, that is one of the improvements in plan and also mentioned in the functional spec. Since we have 3 new auto-failover features to deliver in Vulcan, because of resource and time constraints, plan is to have minimum viable feature in Vulcan and add improvements over time. However, I am thinking of making this particular improvement in Vulcan itself.

Also, there still several small TODOs pending for this feature. This commit has only the core logic. Once it has received +2, there will be several small commits mostly built on top of this commit.
----------------------------------------------------------------------------------------------------------------------
Author: Poonam Dhavale
Date: 2017-12-27 21:48:16.000000000
MISMATCHED INLINE COMMENT
Line:579, src/auto_failover.erl -> Good idea. I have added a check to see whether we are failing over majority of the nodes.
----------------------------------------------------------------------------------------------------------------------
Author: Poonam Dhavale
Date: 2017-12-27 21:48:16.000000000
MISMATCHED INLINE COMMENT
Line:603, src/auto_failover.erl -> Done
----------------------------------------------------------------------------------------------------------------------
Author: Poonam Dhavale
Date: 2017-12-27 21:48:16.000000000
MISMATCHED INLINE COMMENT
Line:163, src/auto_failover_logic.erl -> Yes, I agree. I have changed the state machine. I think the new one is simpler and cleaner. Pleas take a look.
----------------------------------------------------------------------------------------------------------------------
Author: Poonam Dhavale
Date: 2017-12-27 21:48:16.000000000
MISMATCHED INLINE COMMENT
Line:263, src/auto_failover_logic.erl -> Good catch. Fixed.
----------------------------------------------------------------------------------------------------------------------
