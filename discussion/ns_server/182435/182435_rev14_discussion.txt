======================================================================================================================
DESCRIPTION:

MB-54441 Fix broken ram quota restriction in bucket rest api

When a bucket has its ram quota updated immediately after creation of
the bucket, the bucket has a config entry but no servers added to it
yet, which causes the validation to fail, allowing arbritrarily large
ram quota.
The fix is simply to check for an empty server list, and resort to the
kv_nodes list instead.

Change-Id: I57901dae0b164e01224778f67e365327a9cfc1b8

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Peter Searby
Date: 2023-01-09 13:57:55.000000000
Message: 
Uploaded patch set 14: Patch Set 13 was rebased.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2023-01-09 13:58:02.000000000
Message: 
Patch Set 14: -Well-Formed

Build Started https://cv.jenkins.couchbase.com/job/ns-server-test/33193/ (1/2)
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2023-01-09 14:02:32.000000000
Message: 
Patch Set 14:

Build Started https://cv.jenkins.couchbase.com/job/ns-server-ns-test/1054/ (2/2)
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2023-01-09 14:30:14.000000000
Message: 
Patch Set 14: Well-Formed+1

Build Successful 

https://cv.jenkins.couchbase.com/job/ns-server-test/33193/ : SUCCESS

https://cv.jenkins.couchbase.com/job/ns-server-ns-test/1054/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Peter Searby
Date: 2023-01-09 14:30:41.000000000
Message: 
Patch Set 14: Verified+1
----------------------------------------------------------------------------------------------------------------------
Author: Abhijeeth Nuthan
Date: 2023-01-09 20:43:41.000000000
Message: 
Patch Set 14:

(1 comment)
Line:1764, src/menelaus_web_buckets.erl -> In serverless, we can use desired_servers I think, might want to verify it though.
@Artem: would that work.

----------------------------------------------------------------------------------------------------------------------
Author: Peter Searby
Date: 2023-01-13 10:03:15.000000000
Message: 
Patch Set 14: -Verified

(1 comment)
Line:1764, src/menelaus_web_buckets.erl -> get_nodes is only used in two places:
- interpret_ram_quota - which is only called by process_ram_and_storage when width has not been specified, which if I understand correctly means it is not called in serverless?
- additional_bucket_params_validation - this is called in serverless and has one specific case where my current fix doesn't work correctly for serverless, as follows:

If a bucket is created with width 1 on a >1 node cluster and then its durabilityMinLevel is updated to majority, then we would expect to receive the error "You do not have enough data servers to support this durability level".
When sufficient time has passed before the update, the bucket_config has servers populated, so it correctly sees only 1 node, and hence gives the correct message.
When insufficient time has passed, servers is not populated, so KvNodes is checked and so it sees more than 1 node and doesn't give the expected error.

I'll see if checking desired_servers fixes this

----------------------------------------------------------------------------------------------------------------------
Author: Peter Searby
Date: 2023-01-13 11:50:11.000000000
MISMATCHED INLINE COMMENT
Line:1764, src/menelaus_web_buckets.erl -> Checking desired_servers does fix the behaviour, I've updated the patch with this.

To clarify further on my above comment, it would only fix a very specific case where the cluster has more than 1 node but a bucket is assigned to only 1 node despite having the number of replicas set to 1 (which could happen since that's the default value we give it even on a 1 node bucket).
The unexpected behaviour in this case would be that when immediately updating the durabilityMinLevel to something other than none after bucket creation, there would be no error saying that the durability min level is unsupported with the number of data servers.

Is there anything I might be missing @Artem?
----------------------------------------------------------------------------------------------------------------------
Author: Peter Searby
Date: 2023-01-16 17:44:12.000000000
MISMATCHED INLINE COMMENT
Line:1764, src/menelaus_web_buckets.erl -> Done
----------------------------------------------------------------------------------------------------------------------
