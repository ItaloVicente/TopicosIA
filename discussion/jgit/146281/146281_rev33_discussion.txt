======================================================================================================================
DESCRIPTION:

Cache FileStoreAttributeCache per directory

Cache FileStoreAttributeCache entries since looking up FileStore for a
file may be expensive on some platforms.

Implement a simple LRU cache based on ConcurrentHashMap using a simple
long counter to order access to cache entries.

Change-Id: I4881fa938ad2f17712c05da857838073a2fc4ddb
Signed-off-by: Matthias Sohn <matthias.sohn@sap.com>
Signed-off-by: Marc Strapetz <marc.strapetz@syntevo.com>
Also-By: Marc Strapetz <marc.strapetz@syntevo.com>

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Matthias Sohn
Date: 2019-08-05 13:05:54.000000000
Message: 
Uploaded patch set 33.
----------------------------------------------------------------------------------------------------------------------
Author: JGit Bot
Date: 2019-08-05 13:06:03.000000000
Message: 
Patch Set 33:

Build Started https://ci-staging.eclipse.org/jgit/job/stable/job/jgit.gerrit-pipeline/618/
----------------------------------------------------------------------------------------------------------------------
Author: JGit Bot
Date: 2019-08-05 13:15:15.000000000
Message: 
Patch Set 33: Verified+1

Build Successful 

https://ci-staging.eclipse.org/jgit/job/stable/job/jgit.gerrit-pipeline/618/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Matthias Sohn
Date: 2019-08-05 13:39:57.000000000
Message: 
Patch Set 33:

> (1 comment)
 > 
 > Now SimpleLruCache has become a significant performance bottleneck.
 > Why was the implementation thrown all over?

this was an experiment and I intended to run the new benchmark to get some data. Sorry for the noise.

I now ran the benchmark (on Mac OS) for 
* the previous implementation using long counter to track order of cache access
* for the queue based implementation
and for comparison
* using an unbounded ConcurrentHashMap 
* using Caffeine cache

SimpleLruCache long
-------------------

cache size 100, purgefactor 0.5

Benchmark                                         Mode  Cnt    Score   Error  Units
SimpleLruCacheBenchmark.readwrite                 avgt   25  179.049 ± 5.367  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheRead   avgt   25  140.547 ± 5.230  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheWrite  avgt   25  217.551 ± 8.726  ns/op

cache size 1000, purgefactor 0.5

Benchmark                                         Mode  Cnt    Score   Error  Units
SimpleLruCacheBenchmark.readwrite                 avgt   25  175.994 ± 3.514  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheRead   avgt   25  134.095 ± 3.202  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheWrite  avgt   25  217.894 ± 6.684  ns/op

cache size 1000, purgefactor 0.2

Benchmark                                         Mode  Cnt    Score    Error  Units
SimpleLruCacheBenchmark.readwrite                 avgt   25  175.651 ±  8.988  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheRead   avgt   25  131.522 ±  2.496  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheWrite  avgt   25  219.780 ± 17.295  ns/op

cache size 1000, purgefactor 0.1

Benchmark                                         Mode  Cnt    Score    Error  Units
SimpleLruCacheBenchmark.readwrite                 avgt   25  182.600 ±  7.489  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheRead   avgt   25  141.059 ±  3.857  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheWrite  avgt   25  224.141 ± 13.772  ns/op

cache size 1000, purgefactor 0.01

Benchmark                                         Mode  Cnt    Score    Error  Units
SimpleLruCacheBenchmark.readwrite                 avgt   25  190.415 ±  7.057  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheRead   avgt   25  140.447 ±  2.276  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheWrite  avgt   25  240.383 ± 13.432  ns/op

SimpleLruCache queue based
--------------------------

cache size 100

Benchmark                                         Mode  Cnt    Score    Error  Units
SimpleLruCacheBenchmark.readwrite                 avgt   25  432.866 ±  6.785  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheRead   avgt   25  387.493 ±  8.276  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheWrite  avgt   25  478.238 ± 10.541  ns/op

cache size 1000

Benchmark                                         Mode  Cnt    Score    Error  Units
SimpleLruCacheBenchmark.readwrite                 avgt   25  437.496 ±  7.593  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheRead   avgt   25  387.700 ±  5.309  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheWrite  avgt   25  487.292 ± 11.625  ns/op

unbounded ConcurrentHashMap
---------------------------

cache size 100

Benchmark                                         Mode  Cnt    Score   Error  Units
SimpleLruCacheBenchmark.readwrite                 avgt   25   99.736 ± 5.139  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheRead   avgt   25   62.155 ± 6.410  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheWrite  avgt   25  137.316 ± 5.626  ns/op

Caffeine cache
--------------

cache size 1000

Benchmark                                         Mode  Cnt    Score   Error  Units
SimpleLruCacheBenchmark.readwrite                 avgt   25   96.858 ± 8.097  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheRead   avgt   25   65.673 ± 6.988  ns/op
SimpleLruCacheBenchmark.readwrite:testCacheWrite  avgt   25  128.044 ± 9.483  ns/op

This shows that the implementation using long pseudo time to order cache access is faster than the queue based implementation by roughly a factor of 2. The benchmark does not show a 10 times slowdown.
Though maybe there is something wrong with this benchmark, I don't have a lot of experience with benchmarks using JMH.

Hence I reverted to the implementation using long pseudo time to order cache access and changed the default purgeFactor to 0.2 since performance is similar to purgeFactor 0.5 but removes less entries from the cache when purging.

This implementation is slower than unbounded ConcurrentHashMap or Caffeine by roughly a factor of 2 regarding read access and by a factor of 1.7 regarding write access.
----------------------------------------------------------------------------------------------------------------------
Author: Marc Strapetz
Date: 2019-08-06 08:49:21.000000000
Message: 
Patch Set 33:

> I now ran the benchmark (on Mac OS) for 
>     the previous implementation using long counter to track order of cache access
>     for the queue based implementation
>     and for comparison

OK

>     using an unbounded ConcurrentHashMap

What exactly have you been trying here? It's interesting that we seem to have 100% overhead in comparison.

>     using Caffeine cache

I guess the problem isn't important enough to further investigate this route?

Unfortunately I couldn't get the benchmarks running -- mvn complains "[ERROR] Failed to execute goal on project org.eclipse.jgit.benchmarks: Could not resolve dependencies for project org.eclipse.jgit:org.eclipse.jgit.benchmarks:jar:5.1.9-SNAPSHOT: Could not find artifact org.eclipse.jgit:org.eclipse.jgit:jar:5.1.9-SNAPSHOT -> [Help 1]" ... either way:

My benchmarking seems to be quite different: I'm creating a directory tree and then forking many threads which will invoke FS.getFileStoreAttributes() for files from this tree. See code below [1]. The timings I'm referring to are for 1000 files located in 1000 different directories. Tested on Windows (SSD, NTFS, file system cache is inevitably hot).

The ConcurrentLinkedQueue-approach is 10x slower when *not shuffling* the files for each thread. Actually, it's our current approach which becomes significantly better when not shuffling files. This seems to be caused by Files.isDirectory() performance which varies significantly between *shuffling* and *not shuffling* scenarios.

For our current implementation and my benchmark code, Files.isDirectory() becomes the bottleneck. When caching results in FS.FileStoreAttributes.get() not only by directory, but also by Path, this becomes an in-memory-only problem. Now, the implementation of the cache matters significantly more and I see the ConcurrentLinkedQueue-approach to be 1000x slower than our current approach (perfectly reasonable to me). When taking our current approach and just making SimpleLruCache.get and SimpleLruCache.put synchronized, this will make it ~10x slower (quite reasonable to me).

Summarizing:

(1) our approach is ~3x faster compared to the ConcurrentLinkedQueue-approach

(2) our approach is not faster compared to a simple, synchronized Map

(3) with additional caching in FS, our approach is ~10x faster compared to a simple, synchronized Map and ~1000x faster compared to the ConcurrentLinkedQueue-approach 

Conclusion:
I would keep things as they are now; especially not address (3) and also stay away from the simple, synchronized Map (2) because I agree with Matthias' concerns in a highly multi-threaded environment, like Gerrit.

[1]

    package org.eclipse.jgit.util;

    import java.io.IOException;
    import java.nio.file.Files;
    import java.nio.file.Path;
    import java.util.ArrayList;
    import java.util.Collections;
    import java.util.List;
    import java.util.concurrent.CountDownLatch;

    public class FSMain1c {

        public static void main(String[] args) throws IOException, InterruptedException {
            System.out.println("Setting up benchmark playground");

            final int dirCount = 1000;
            final int fileCount = 1;
            final int cacheSize = 10000;
            final int threadCount = 100;
            final long runningTimeMillis = 300000;
            FS.FileStoreAttributes.configureAttributesPathCache(cacheSize, 0.25f);

            final Path root = Files.createTempDirectory("jgit-fs-benchmark");
            final List<Path> dirs = new ArrayList<>();
            final List<Path> files = new ArrayList<>();
            for (int dirIndex = 0; dirIndex < dirCount; dirIndex++) {
                final Path dir = Files.createDirectory(root.resolve("dir" + dirIndex));
                dirs.add(dir);

                for (int fileIndex = 0; fileIndex < fileCount; fileIndex++) {
                    final Path file = Files.createFile(dir.resolve("file" + dirIndex + "." + fileIndex));
                    files.add(file);
                }
            }

            final List<MyThread> threads = new ArrayList<>();
            for (int index = 0; index < threadCount; index++) {
                final MyThread thread = new MyThread(index);

                thread.start();
                threads.add(thread);
            }

            System.out.println("Starting benchmark");

            try {
                final long benchmarkStart = System.currentTimeMillis();
                long durationSum = 0;
                long durationCount = 0;
                for (int loops = 0; ; loops++) {
                    final List<List<Path>> shuffledPaths = new ArrayList<>();
                    for (int threadIndex = 0; threadIndex < threads.size(); threadIndex++) {
                        final List<Path> threadFiles = new ArrayList<>(files);
                        Collections.shuffle(threadFiles);
                        shuffledPaths.add(threadFiles);
                    }

                    final long start = System.nanoTime();
                    final CountDownLatch countDownLatch = new CountDownLatch(threads.size());
                    for (int threadIndex = 0; threadIndex < threads.size(); threadIndex++) {
                        MyThread thread = threads.get(threadIndex);
                        thread.files = shuffledPaths.get(threadIndex);
                        thread.countDownLatch = countDownLatch;
                        synchronized (thread.semaphore) {
                            thread.semaphore.notifyAll();
                        }
                    }
                    countDownLatch.await();

                    final long end = System.nanoTime();
                    final long duration = (end - start) / 1000000L;
                    if (loops >= 3) {
                        durationSum += duration;
                        durationCount++;
                    }
                    System.out.println(duration + "ms " + (durationSum > 0 ? "(" + durationSum / durationCount + " ms avg)" : ""));

                    if (System.currentTimeMillis() - benchmarkStart > runningTimeMillis) {
                        break;
                    }
                }
            } finally {
                System.out.println("Cleaning up benchmark playground");

                for (MyThread thread : threads) {
                    thread.shutdown = true;
                    synchronized (thread.semaphore) {
                        thread.semaphore.notifyAll();
                    }
                }

                for (Path file : files) {
                    Files.delete(file);
                }
                for (Path dir : dirs) {
                    Files.delete(dir);
                }
                Files.delete(root);
            }
        }

        private static class MyThread extends Thread {
            private final Object semaphore = new Object();
            private volatile List<Path> files;
            private volatile CountDownLatch countDownLatch;
            private volatile boolean shutdown;

            MyThread(int index) {
                super(String.valueOf(index));
            }

            @Override
            public void run() {
                for (; ; ) {
                    while (files == null) {
                        try {
                            synchronized (semaphore) {
                                semaphore.wait();
                            }
                        } catch (InterruptedException e) {
                            e.printStackTrace();
                        }

                        if (shutdown) {
                            return;
                        }
                    }
                    for (Path dir : files) {
                        FS.getFileStoreAttributes(dir);
                    }
                    files = null;
                    countDownLatch.countDown();
                }
            }
        }
    }
----------------------------------------------------------------------------------------------------------------------
Author: Matthias Sohn
Date: 2019-08-06 09:19:14.000000000
Message: 
Patch Set 33:

> > I now ran the benchmark (on Mac OS) for
 > >     the previous implementation using long counter to track order
 > of cache access
 > >     for the queue based implementation
 > >     and for comparison
 > 
 > OK
 > 
 > >     using an unbounded ConcurrentHashMap
 > 
 > What exactly have you been trying here? It's interesting that we
 > seem to have 100% overhead in comparison.

private volatile Map<String, String> cache = new ConcurrentHashMap<>();

and replace return type of benchmark methods to Map<String, String>

 > 
 > >     using Caffeine cache
 > 
 > I guess the problem isn't important enough to further investigate
 > this route?
 
if this cache becomes a problem in large scale server applications like Gerrit
we can still replace this simple cache by Caffeine at the price of adding another dependency.

 > Unfortunately I couldn't get the benchmarks running -- mvn
 > complains "[ERROR] Failed to execute goal on project
 > org.eclipse.jgit.benchmarks: Could not resolve dependencies for
 > project org.eclipse.jgit:org.eclipse.jgit.benchmarks:jar:5.1.9-SNAPSHOT:
 > Could not find artifact org.eclipse.jgit:org.eclipse.jgit:jar:5.1.9-SNAPSHOT
 > -> [Help 1]" ... either way:

you first need to run the build from the jgit root directory
and install the resulting artefacts in the local maven repo:
$ mvn clean install -DskipTests -Dmaven.javadoc.skip=true
then you should be able to run the benchmark
$ java -jar org.eclipse.jgit.benchmarks/target/benchmarks.jar

 > My benchmarking seems to be quite different: I'm creating a
 > directory tree and then forking many threads which will invoke
 > FS.getFileStoreAttributes() for files from this tree. See code
 > below [1]. The timings I'm referring to are for 1000 files located
 > in 1000 different directories. Tested on Windows (SSD, NTFS, file
 > system cache is inevitably hot).
 
If you don't mind I'll try to turn this into another JMH benchmark

 > The ConcurrentLinkedQueue-approach is 10x slower when *not
 > shuffling* the files for each thread. Actually, it's our current
 > approach which becomes significantly better when not shuffling
 > files. This seems to be caused by Files.isDirectory() performance
 > which varies significantly between *shuffling* and *not shuffling*
 > scenarios.
 > 
 > For our current implementation and my benchmark code,
 > Files.isDirectory() becomes the bottleneck. When caching results in
 > FS.FileStoreAttributes.get() not only by directory, but also by
 > Path, this becomes an in-memory-only problem. Now, the
 > implementation of the cache matters significantly more and I see
 > the ConcurrentLinkedQueue-approach to be 1000x slower than our
 > current approach (perfectly reasonable to me). When taking our
 > current approach and just making SimpleLruCache.get and
 > SimpleLruCache.put synchronized, this will make it ~10x slower
 > (quite reasonable to me).
 > 
 > Summarizing:
 > 
 > (1) our approach is ~3x faster compared to the ConcurrentLinkedQueue-approach
 > 
 > (2) our approach is not faster compared to a simple, synchronized
 > Map
 > 
 > (3) with additional caching in FS, our approach is ~10x faster
 > compared to a simple, synchronized Map and ~1000x faster compared
 > to the ConcurrentLinkedQueue-approach
 > 
 > Conclusion:
 > I would keep things as they are now; especially not address (3) and
 > also stay away from the simple, synchronized Map (2) because I
 > agree with Matthias' concerns in a highly multi-threaded
 > environment, like Gerrit.
 
+1

 > [1]
 > 
 > package org.eclipse.jgit.util;
 > 
 > import java.io.IOException;
 > import java.nio.file.Files;
 > import java.nio.file.Path;
 > import java.util.ArrayList;
 > import java.util.Collections;
 > import java.util.List;
 > import java.util.concurrent.CountDownLatch;
 > 
 > public class FSMain1c {
 > 
 > public static void main(String[] args) throws IOException,
 > InterruptedException {
 > System.out.println("Setting up benchmark playground");
 > 
 > final int dirCount = 1000;
 > final int fileCount = 1;
 > final int cacheSize = 10000;
 > final int threadCount = 100;
 > final long runningTimeMillis = 300000;
 > FS.FileStoreAttributes.configureAttributesPathCache(cacheSize,
 > 0.25f);
 > 
 > final Path root = Files.createTempDirectory("jgit-fs-benchmark");
 > final List<Path> dirs = new ArrayList<>();
 > final List<Path> files = new ArrayList<>();
 > for (int dirIndex = 0; dirIndex < dirCount; dirIndex++) {
 > final Path dir = Files.createDirectory(root.resolve("dir" +
 > dirIndex));
 > dirs.add(dir);
 > 
 > for (int fileIndex = 0; fileIndex < fileCount; fileIndex++) {
 > final Path file = Files.createFile(dir.resolve("file" + dirIndex +
 > "." + fileIndex));
 > files.add(file);
 > }
 > }
 > 
 > final List<MyThread> threads = new ArrayList<>();
 > for (int index = 0; index < threadCount; index++) {
 > final MyThread thread = new MyThread(index);
 > 
 > thread.start();
 > threads.add(thread);
 > }
 > 
 > System.out.println("Starting benchmark");
 > 
 > try {
 > final long benchmarkStart = System.currentTimeMillis();
 > long durationSum = 0;
 > long durationCount = 0;
 > for (int loops = 0; ; loops++) {
 > final List<List<Path>> shuffledPaths = new ArrayList<>();
 > for (int threadIndex = 0; threadIndex < threads.size();
 > threadIndex++) {
 > final List<Path> threadFiles = new ArrayList<>(files);
 > Collections.shuffle(threadFiles);
 > shuffledPaths.add(threadFiles);
 > }
 > 
 > final long start = System.nanoTime();
 > final CountDownLatch countDownLatch = new CountDownLatch(threads.size());
 > for (int threadIndex = 0; threadIndex < threads.size();
 > threadIndex++) {
 > MyThread thread = threads.get(threadIndex);
 > thread.files = shuffledPaths.get(threadIndex);
 > thread.countDownLatch = countDownLatch;
 > synchronized (thread.semaphore) {
 > thread.semaphore.notifyAll();
 > }
 > }
 > countDownLatch.await();
 > 
 > final long end = System.nanoTime();
 > final long duration = (end - start) / 1000000L;
 > if (loops >= 3) {
 > durationSum += duration;
 > durationCount++;
 > }
 > System.out.println(duration + "ms " + (durationSum > 0 ? "(" +
 > durationSum / durationCount + " ms avg)" : ""));
 > 
 > if (System.currentTimeMillis() - benchmarkStart > runningTimeMillis)
 > {
 > break;
 > }
 > }
 > } finally {
 > System.out.println("Cleaning up benchmark playground");
 > 
 > for (MyThread thread : threads) {
 > thread.shutdown = true;
 > synchronized (thread.semaphore) {
 > thread.semaphore.notifyAll();
 > }
 > }
 > 
 > for (Path file : files) {
 > Files.delete(file);
 > }
 > for (Path dir : dirs) {
 > Files.delete(dir);
 > }
 > Files.delete(root);
 > }
 > }
 > 
 > private static class MyThread extends Thread {
 > private final Object semaphore = new Object();
 > private volatile List<Path> files;
 > private volatile CountDownLatch countDownLatch;
 > private volatile boolean shutdown;
 > 
 > MyThread(int index) {
 > super(String.valueOf(index));
 > }
 > 
 > @Override
 > public void run() {
 > for (; ; ) {
 > while (files == null) {
 > try {
 > synchronized (semaphore) {
 > semaphore.wait();
 > }
 > } catch (InterruptedException e) {
 > e.printStackTrace();
 > }
 > 
 > if (shutdown) {
 > return;
 > }
 > }
 > for (Path dir : files) {
 > FS.getFileStoreAttributes(dir);
 > }
 > files = null;
 > countDownLatch.countDown();
 > }
 > }
 > }
 > }

I am now testing this series on a beefy Linux server with different Linux file systems (btrfs, ext4, xfs, zfs)
using Java 8 and Java 11 to get some more coverage of OS, filesystem and Java combinations
----------------------------------------------------------------------------------------------------------------------
