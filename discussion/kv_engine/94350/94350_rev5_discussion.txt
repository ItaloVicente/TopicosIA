======================================================================================================================
DESCRIPTION:

MB-25822: Create separate mutex for pending_io

This is a recreate of commit 0800742b against our spock branch. Lots
of refactoring has occurred between spock to vulcan, meaning a simple
cherry-pick of the original patch was not easy.

This patch recreates the original intent and switches the pending-io
from a list to std::map (rather than the original switch from std::set
to std::map)

The LIBEVENT_THREAD::mutex currently guards the whole state of the
worker thread - as such it is acquired by the worker thread when it
starts processing event(s) for a connection, and only unlocked when it
finishes with that connection.

The state guarded includes the pending_io set; which is (primarily):

a) Read at the start of the event loop (to re-register connections in
   libevent).

b) Written by notify_io_complete when background threads want to
   wakeup frontend threads.

Profiling of the LIBEVENT_THREAD::mutex wait/lock times shows that
callers of notify_io_complete (notably ConnManager and DCP
ConnNotifier task) can end up waiting for long periods (~100ms in
local testing) to simply add a connection to the pending set.

To improve this, add a separate mutex which is only used for the
pending_io set. This also requires a change to the data structure for
the pending_io - we need to additionally record the pending status
code against each connection.

This is because previously this status (aiostat) was updated with
LIBEVENT_THREAD::mutex held; therefore no worker thread could possibly
be running. With the separate mutex this is no longer the case and we
cannot simply write to aiostat as it may be being accessed by the
worker. We obviously don't want to have to acquire
LIBEVENT_THREAD::mutex to update it as that introduces the same
problem. Instead we record the pending status with the connection in
pending_io; and when we come to process pending_io in the worker
thread (which already holds LIBEVENT_THREAD::mutex) we update aiostat
with the pending status before running it's event loop).

Change-Id: Ifad49f12d2249b92d4ffd3a761e353bb85d0d2d5

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Jim Walker
Date: 2018-05-18 11:33:22.000000000
Message: 
Uploaded patch set 5.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2018-05-18 11:33:30.000000000
Message: 
Patch Set 5: Well-Formed+1

Permission granted to commit: 

http://server.jenkins.couchbase.com/job/restricted-branch-check/90303/artifact/restricted.html : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2018-05-18 11:49:46.000000000
Message: 
Patch Set 5: Verified-1

Build Failed 

http://cv.jenkins.couchbase.com/job/kv_engine-linux-spock/988/ : ABORTED

No problems were identified. If you know why this problem occurred, please add a suitable Cause for it. ( http://cv.jenkins.couchbase.com/job/kv_engine-linux-spock/988/ )

http://cv.jenkins.couchbase.com/job/kv_engine-threadsanitizer-spock/925/ : ABORTED

No problems were identified. If you know why this problem occurred, please add a suitable Cause for it. ( http://cv.jenkins.couchbase.com/job/kv_engine-threadsanitizer-spock/925/ )

http://cv.jenkins.couchbase.com/job/kv_engine-windows-spock/959/ : FAILURE

Failure of a GoogleTest  ( http://cv.jenkins.couchbase.com/job/kv_engine-windows-spock/959/ )

Failure of a CTest test  1/95 Test #85: memcached-shutdown-tests ....................... ( http://cv.jenkins.couchbase.com/job/kv_engine-windows-spock/959/ )

http://cv.jenkins.couchbase.com/job/kv_engine-clang_format/8111/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-addresssanitizer-spock/933/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
