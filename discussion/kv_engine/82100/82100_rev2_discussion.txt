======================================================================================================================
DESCRIPTION:

MB-25179: Fix write buffer management

There was a bug in handling of the write buffers when trying
to determinate if the buffer was still in use or not. It would
manifest itself under high write load to the socket from the
memcached side.

Each connection would use the write buffer object as a scratch
space where it could format data into, and then record the
offset into the buffer into the io-vector for data to send
over the network by using sendmsg. So far everything sounds
good, but at one point we wanted to reduce the resource usage
from idle connections by "stealing" the read and write buffer
from the connection if there wasn't any pending data in the
buffer. For the read buffer this check was correct, but
we had code paths in the write buffer path which wouldn't
update the number of bytes it had stored in the buffer.
Under a "normal" load this wouldn't fail, as we normally only
used 24ish bytes in the buffer (the protocol header), and
the socket would accept this chunk of memory (but possibly
not the large value following located in _another_ buffer
pinned to the request and not the write buffer).

With this patch we track how much data we format into the buffer,
and how much data we read out of the buffer so that we know for
sure when we're done with the buffer.

Run on (8 X 2500 MHz CPU s)
2017-08-09 07:15:37
---------------------------------------------------------------------
Benchmark                              Time           CPU Iterations
---------------------------------------------------------------------
PipeClearBenchmark                     2 ns          2 ns  345516918
PipeProducedAndClearBenchmark          2 ns          2 ns  346979543
McbpAddHeaderBenchmark                 6 ns          6 ns  110382238
InitializeMsgHeaderBenchmark           2 ns          2 ns  353144754
AdjustMsgHeaderBenchmark               9 ns          9 ns   73386031

If we compare these to the previous implementation:

Run on (8 X 2500 MHz CPU s)
2017-08-08 22:18:35
--------------------------------------------------------------------
Benchmark                             Time           CPU Iterations
--------------------------------------------------------------------
McbpAddHeaderBenchmark                4 ns          4 ns  162504991
InitializeMsgHeaderBenchmark          2 ns          2 ns  370970990
AdjustMsgHeaderBenchmark              7 ns          7 ns   94592038

McbpAddHeaderBenchmark increase from 4ns to 6ns, but in this
implementation we need to subtract PipeClearBenchmark from the
result as we need to reset the pipe for each invocation (to avoid
the data to grow infinitely). That results in "zero" difference.

Likewise the AdjustMsgHeaderBenchmark grows from 7ns to 9ns, and
here we need to subtract PipeProducedAndClearBenchmark as that is
also needed to make the microbenchmark work. That results in
the same execution speed.

Change-Id: I0184efb6d960df6b5ff60ff29386927aca046b6e

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Trond Norbye
Date: 2017-08-09 06:02:24.000000000
Message: 
Uploaded patch set 2.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2017-08-09 06:22:33.000000000
Message: 
Patch Set 2: Verified+1

Build Successful 

http://cv.jenkins.couchbase.com/job/kv_engine-addresssanitizer-master/1775/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-linux-master/1825/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/1819/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-threadsanitizer-master/1854/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/1646/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
