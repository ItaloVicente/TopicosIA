======================================================================================================================
DESCRIPTION:

MB-45523, MB-44877: Add send-queue-size in connection stats

Change-Id: Idcdb4f68d56078e42ad2881af9a56b90ae5f410c

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Paolo Cocchi
Date: 2021-04-12 14:26:59.000000000
Message: 
Uploaded patch set 1.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2021-04-12 14:39:58.000000000
Message: 
Patch Set 1: Verified-1

Build Failed 

http://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/29947/ : FAILURE

Compile error at /home/couchbase/jenkins/workspace/kv_engine-clang_analyzer-master/kv_engine/engines/ep/tests/module_tests/dcp_test.cc:1561:7:
error: variable type "class MockServerCookieApi" is an abstract class
 ( http://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/29947/ )

http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/34893/ : FAILURE

Compile error at ..\kv_engine\engines\ep\tests\module_tests\dcp_test.cc(1561): error C2259: "ConnectionTest_test_mb20716_connmap_notify_on_delete_Test::TestBody::MockServerCookieApi": cannot instantiate abstract class ( http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/34893/ )

http://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/9949/ : FAILURE

Compile error at [2021-04-12T14:32:27.412Z] ../kv_engine/engines/ep/tests/module_tests/dcp_test.cc:1561:7:
error: cannot declare variable ?scapi? to be of abstract type ?ConnectionTest_test_mb20716_connmap_notify_on_delete_Test::TestBody()::MockServerCookieApi?
 ( http://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/9949/ )

http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/8881/ : FAILURE

Compile error at [2021-04-12T14:29:31.641Z] ../kv_engine/engines/ep/tests/module_tests/dcp_test.cc:1561:7:
error: variable type "class MockServerCookieApi" is an abstract class
 ( http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/8881/ )

http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/9812/ : FAILURE

Compile error at [2021-04-12T14:30:44.574Z] ../kv_engine/engines/ep/tests/module_tests/dcp_test.cc:1561:7:
error: cannot declare variable ?scapi? to be of abstract type ?ConnectionTest_test_mb20716_connmap_notify_on_delete_Test::TestBody()::MockServerCookieApi?
 ( http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/9812/ )

http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/10180/ : FAILURE

Compile error at [2021-04-12T14:29:44.573Z] ../kv_engine/engines/ep/tests/module_tests/dcp_test.cc:1561:7:
error: variable type "class MockServerCookieApi" is an abstract class
 ( http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/10180/ )

http://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/master/17728/ : FAILURE

Compile error at [2021-04-12T14:35:21.823Z] ../kv_engine/engines/ep/tests/module_tests/dcp_test.cc:1561:7:
error: variable type "class MockServerCookieApi" is an abstract class
 ( http://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/master/17728/ )

http://cv.jenkins.couchbase.com/job/kv_engine-clang_tidy/8291/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_format_9/6836/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2021-04-12 15:54:03.000000000
Message: 
Patch Set 1:

Not sure what you want to achieve here.. the way it currently work is that when we get a callback from livevent we start processing command.. All of these commands will put their response into the send queue (they won't be tried to sent immediately)... then for DCP connections we start calling step until we hit the watermark (or the engine tells us to stop).

When we return from our callback code livevent will start sending the data for the connection.. if we get a notification on the socket (for instane more data is available) we'll start copying into that buffer again.

so it would "never" be 0...
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2021-04-13 08:52:34.000000000
Message: 
Patch Set 1:

> Patch Set 1:
> 
> Not sure what you want to achieve here.. the way it currently work is that when we get a callback from livevent we start processing command.. All of these commands will put their response into the send queue (they won't be tried to sent immediately)... then for DCP connections we start calling step until we hit the watermark (or the engine tells us to stop).
> 
> When we return from our callback code livevent will start sending the data for the connection.. if we get a notification on the socket (for instane more data is available) we'll start copying into that buffer again.
> 
> so it would "never" be 0...

The purpose is exposing the current send-queue-size for having a sign in situations (eg, low bandwidth like) where the connection hits '!more' in Connection::executeCommandsCallback.

MB-45523 is an example. Stream::readyQ huge, connection never paused (bulk load and in-memory stream pulling from the checkpoint), but still DCP throughput < 1 item/s 'cause of a slow connection link.

We have no evidence of that cause in out logs/stats. But if we had the possibility of watching the

  ep_dcpq::conn-name::send_queue_size

we would probably see that it is constantly very close to the SendQMaxSize, thus !more.
----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2021-04-13 11:29:09.000000000
Message: 
Patch Set 1:

> Patch Set 1:
> 
> > Patch Set 1:
> > 
> > Not sure what you want to achieve here.. the way it currently work is that when we get a callback from livevent we start processing command.. All of these commands will put their response into the send queue (they won't be tried to sent immediately)... then for DCP connections we start calling step until we hit the watermark (or the engine tells us to stop).
> > 
> > When we return from our callback code livevent will start sending the data for the connection.. if we get a notification on the socket (for instane more data is available) we'll start copying into that buffer again.
> > 
> > so it would "never" be 0...
> 
> The purpose is exposing the current send-queue-size for having a sign in situations (eg, low bandwidth like) where the connection hits '!more' in Connection::executeCommandsCallback.
> 
> MB-45523 is an example. Stream::readyQ huge, connection never paused (bulk load and in-memory stream pulling from the checkpoint), but still DCP throughput < 1 item/s 'cause of a slow connection link.
> 
> We have no evidence of that cause in out logs/stats. But if we had the possibility of watching the
> 
>   ep_dcpq::conn-name::send_queue_size
> 
> we would probably see that it is constantly very close to the SendQMaxSize, thus !more.

Wouldn't it be better to log slow draining connections in the core (to make sure it also get logged for all sort of traffic and not just DCP by explicit polling of stats?)
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2021-04-13 11:37:49.000000000
Message: 
Patch Set 1:

> Patch Set 1:
> 
> > Patch Set 1:
> > 
> > > Patch Set 1:
> > > 
> > > Not sure what you want to achieve here.. the way it currently work is that when we get a callback from livevent we start processing command.. All of these commands will put their response into the send queue (they won't be tried to sent immediately)... then for DCP connections we start calling step until we hit the watermark (or the engine tells us to stop).
> > > 
> > > When we return from our callback code livevent will start sending the data for the connection.. if we get a notification on the socket (for instane more data is available) we'll start copying into that buffer again.
> > > 
> > > so it would "never" be 0...
> > 
> > The purpose is exposing the current send-queue-size for having a sign in situations (eg, low bandwidth like) where the connection hits '!more' in Connection::executeCommandsCallback.
> > 
> > MB-45523 is an example. Stream::readyQ huge, connection never paused (bulk load and in-memory stream pulling from the checkpoint), but still DCP throughput < 1 item/s 'cause of a slow connection link.
> > 
> > We have no evidence of that cause in out logs/stats. But if we had the possibility of watching the
> > 
> >   ep_dcpq::conn-name::send_queue_size
> > 
> > we would probably see that it is constantly very close to the SendQMaxSize, thus !more.
> 
> Wouldn't it be better to log slow draining connections in the core (to make sure it also get logged for all sort of traffic and not just DCP by explicit polling of stats?)

Sure we can log something at memcached lavel but we need to handle not blowing logs etc.., so it's sounds more an addition than a replacement to a conn stat
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2021-04-13 11:38:35.000000000
Message: 
Patch Set 1:

> Patch Set 1:
> 
> > Patch Set 1:
> > 
> > > Patch Set 1:
> > > 
> > > > Patch Set 1:
> > > > 
> > > > Not sure what you want to achieve here.. the way it currently work is that when we get a callback from livevent we start processing command.. All of these commands will put their response into the send queue (they won't be tried to sent immediately)... then for DCP connections we start calling step until we hit the watermark (or the engine tells us to stop).
> > > > 
> > > > When we return from our callback code livevent will start sending the data for the connection.. if we get a notification on the socket (for instane more data is available) we'll start copying into that buffer again.
> > > > 
> > > > so it would "never" be 0...
> > > 
> > > The purpose is exposing the current send-queue-size for having a sign in situations (eg, low bandwidth like) where the connection hits '!more' in Connection::executeCommandsCallback.
> > > 
> > > MB-45523 is an example. Stream::readyQ huge, connection never paused (bulk load and in-memory stream pulling from the checkpoint), but still DCP throughput < 1 item/s 'cause of a slow connection link.
> > > 
> > > We have no evidence of that cause in out logs/stats. But if we had the possibility of watching the
> > > 
> > >   ep_dcpq::conn-name::send_queue_size
> > > 
> > > we would probably see that it is constantly very close to the SendQMaxSize, thus !more.
> > 
> > Wouldn't it be better to log slow draining connections in the core (to make sure it also get logged for all sort of traffic and not just DCP by explicit polling of stats?)
> 
> Sure we can log something at memcached lavel but we need to handle not blowing logs etc.., so it's sounds more an addition than a replacement to a conn stat

Also, a conn stats will be visible in tools like mortimer, so very handy at diagnostic
----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2021-04-13 12:53:00.000000000
Message: 
Patch Set 1:

> Patch Set 1:
> 
> > Patch Set 1:
> > 
> > > Patch Set 1:
> > > 
> > > > Patch Set 1:
> > > > 
> > > > > Patch Set 1:
> > > > > 
> > > > > Not sure what you want to achieve here.. the way it currently work is that when we get a callback from livevent we start processing command.. All of these commands will put their response into the send queue (they won't be tried to sent immediately)... then for DCP connections we start calling step until we hit the watermark (or the engine tells us to stop).
> > > > > 
> > > > > When we return from our callback code livevent will start sending the data for the connection.. if we get a notification on the socket (for instane more data is available) we'll start copying into that buffer again.
> > > > > 
> > > > > so it would "never" be 0...
> > > > 
> > > > The purpose is exposing the current send-queue-size for having a sign in situations (eg, low bandwidth like) where the connection hits '!more' in Connection::executeCommandsCallback.
> > > > 
> > > > MB-45523 is an example. Stream::readyQ huge, connection never paused (bulk load and in-memory stream pulling from the checkpoint), but still DCP throughput < 1 item/s 'cause of a slow connection link.
> > > > 
> > > > We have no evidence of that cause in out logs/stats. But if we had the possibility of watching the
> > > > 
> > > >   ep_dcpq::conn-name::send_queue_size
> > > > 
> > > > we would probably see that it is constantly very close to the SendQMaxSize, thus !more.
> > > 
> > > Wouldn't it be better to log slow draining connections in the core (to make sure it also get logged for all sort of traffic and not just DCP by explicit polling of stats?)
> > 
> > Sure we can log something at memcached lavel but we need to handle not blowing logs etc.., so it's sounds more an addition than a replacement to a conn stat
> 
> Also, a conn stats will be visible in tools like mortimer, so very handy at diagnostic

perhaps better to log the total recv / sent and not the current send buffer?
----------------------------------------------------------------------------------------------------------------------
