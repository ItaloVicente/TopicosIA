======================================================================================================================
DESCRIPTION:

MB-50984: Remove num-item-based checkpoint creation

Since Neo we have an implicit upper bound on the max number of
checkpoints (per bucket) that is logically given by (CMQuota /
CheckpointMaxSize).

Still, there are code paths that interfere with that and possibly break
that invariant. One of those is chk_max_items, which triggers checkpoint
creation when the user-defined max num of items has been queued into a
single checkpoint. Here we remove that trigger.

Change-Id: I2c49065a3d1b05493df8ce9865982d6393f9373f

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Paolo Cocchi
Date: 2022-04-27 14:00:01.000000000
Message: 
Uploaded patch set 16: Patch Set 15 was rebased.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2022-04-27 15:02:40.000000000
Message: 
Patch Set 16: Verified-1

Build Failed 

http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/45733/ : FAILURE

Failure of an engine_testapp test:

<pre>
Running [24/93]: test producer stream request (disk only)...(331 ms) OK


99% tests passed, 2 tests failed out of 427

Total Test time (real) = 793.11 sec

The following tests FAIL
</pre>
 ( http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/45733/ )

Timeout of a CTest test 427/427 Test #290: ep_testsuite_dcp.value_eviction.comp_passive ............................................................} ( http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/45733/ )

http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/18863/ : FAILURE

No problems were identified. If you know why this problem occurred, please add a suitable Cause for it. ( http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/18863/ )

http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/18003/ : FAILURE

Failure of an engine_testapp test:

<pre>
Running [42/160]: file stats post warmup...../kv_engine/engines/ep/tests/ep_testsuite.cc:2327 Test failed: `" (Expected `376013" to be less than or equal to `368640" - Unexpected fileSize for vbucket)
[2022-04-27T14:34:38.670Z] (250 ms) FAIL
</pre>
 ( http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/18003/ )

http://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/38414/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_tidy-master/17077/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_format_9/16046/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/master/27279/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/19905/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.libFuzzer/job/master/6734/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.aarch64-linux/job/master/6827/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/19207/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2022-04-28 06:15:02.000000000
Message: 
Patch Set 16:

Build Failed 

http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/18891/ : FAILURE

Failure of GoogleTest "TransportProtocols/GetSetTest.ServerRejectsLargeSizeWithXattrCompressed/McbpSsl_XattrYes_JsonYes_SnappyYes":

<pre>
[ RUN      ] TransportProtocols/GetSetTest.ServerRejectsLargeSizeWithXattrCompressed/McbpSsl_XattrYes_JsonYes_SnappyYes
[2022-04-28T06:14:32.865Z] unknown file: Failure
[2022-04-28T06:14:32.865Z] C++ exception with description "Failed to mutateWithMeta TransportProtocols_GetSetTest_ServerRejectsLargeSizeWithXattrCompressed_McbpSsl_XattrYes_JsonYes_SnappyYes : Temporary failure (134)" thrown in the test body.
[2022-04-28T06:14:32.865Z] [  FAILED  ] TransportProtocols/GetSetTest.ServerRejectsLargeSizeWithXattrCompressed/McbpSsl_XattrYes_JsonYes_SnappyYes, where GetParam() = (McbpSsl, XattrYes, 4-byte object <00-00 00-00>, 4-byte object <00-00 00-00>) (209 ms)
TransportProtocols/GetSetTest.ServerRejectsLargeSizeWithXattrCompressed/McbpSsl_XattrYes_JsonYes_SnappyYes
</pre>
 ( http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/18891/ )

http://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/38414/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_tidy-master/17077/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_format_9/16046/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/master/27279/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/19905/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.libFuzzer/job/master/6734/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/18029/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.aarch64-linux/job/master/6827/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/19207/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/45760/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2022-04-28 07:13:48.000000000
Message: 
Patch Set 16:

Build Failed 

http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/18897/ : FAILURE

Failure of GoogleTest "TransportProtocols/GetSetTest.ServerRejectsLargeSizeWithXattrCompressed/McbpSsl_XattrYes_JsonYes_SnappyYes":

<pre>
[ RUN      ] TransportProtocols/GetSetTest.ServerRejectsLargeSizeWithXattrCompressed/McbpSsl_XattrYes_JsonYes_SnappyYes
[2022-04-28T07:09:06.522Z] unknown file: Failure
[2022-04-28T07:09:06.522Z] C++ exception with description "Failed to mutateWithMeta TransportProtocols_GetSetTest_ServerRejectsLargeSizeWithXattrCompressed_McbpSsl_XattrYes_JsonYes_SnappyYes : Temporary failure (134)" thrown in the test body.
[2022-04-28T07:09:06.522Z] [  FAILED  ] TransportProtocols/GetSetTest.ServerRejectsLargeSizeWithXattrCompressed/McbpSsl_XattrYes_JsonYes_SnappyYes, where GetParam() = (McbpSsl, XattrYes, 4-byte object <00-00 00-00>, 4-byte object <00-00 00-00>) (196 ms)
TransportProtocols/GetSetTest.ServerRejectsLargeSizeWithXattrCompressed/McbpSsl_XattrYes_JsonYes_SnappyYes
</pre>
 ( http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/18897/ )

http://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/38414/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_tidy-master/17077/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_format_9/16046/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/master/27279/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/19905/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.libFuzzer/job/master/6734/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/18029/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.aarch64-linux/job/master/6827/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/19207/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/45760/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2022-04-28 11:41:26.000000000
Message: 
Patch Set 16: Verified+1

Build Successful 

http://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/38414/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_tidy-master/17077/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_format_9/16046/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/master/27279/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/19905/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.libFuzzer/job/master/6734/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/18029/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.aarch64-linux/job/master/6827/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/19207/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/45760/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/18922/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2022-05-04 14:44:09.000000000
Message: 
Patch Set 16:

(1 comment)
File Comment: /PATCHSET_LEVEL -> How many items do we now effectively allow in a Checkpoint? I guess we have an implicit cap which is the individual Checkpoint quota / minimum item + size of some small key. Is it possible that we end up with an order of magnitude more mutations in a given Checkpoint with minimal sized keys + Items?  Any concerns of performance impact in such a case?
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2022-05-09 14:22:25.000000000
Message: 
Patch Set 16:

(1 comment)
File Comment: /PATCHSET_LEVEL -> > How many items do we now effectively allow in a Checkpoint? I guess we have an implicit cap which is the individual Checkpoint quota / minimum item + size of some small key

Correct, theoretical max num items in a checkpoint is being checkpoint_max_size_bytes/avg_item_alloc_in_ckpt.

>  Is it possible that we end up with an order of magnitude more mutations in a given Checkpoint with minimal sized keys + Items?

Considering that before the change we allow 10'000 items by default (up to 100'000 by config), then in practice no. Note that everything is accounted against the quota. Ie, avg_item_alloc_in_ckpt above includes the mem-overhead for the keyIndex, etc..

> Any concerns of performance impact in such a case?

I've asked myself the same question that you're probably asking yourself. If for some reason we end up with much more items than expected in the single checkpoint (which isn't expected to happen actually for what I said before), then is there any perf degradation anywhere that we can expect?

I answered myself with considering the worst case of a crazy number of items in a checkpoint (again, not expected in practice):
1. All the cursor logic is linear by definition, regardless of whether items are spread across fewer/more checkpoint
2. For the flush batch we are interested in the size-in-bytes (not size-in-num-items)
3. Maybe the single ItemExpel run might be impacted in the runtime ? But (a) the "extraction" phase is O(1) and (b) deallocation (although linear) is performed lock-free. So in the end that would be a runtime increase difference given only by the fact that we are expelling more in a single expel run

I couldn't find any critical path even by considering a worst-case (and not expected) scenario
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2022-05-10 09:42:23.000000000
Message: 
Patch Set 16:

(1 comment)
File Comment: /PATCHSET_LEVEL -> @ben any thought on the above?
also @dave do you have extra concern on this?
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2022-05-10 10:15:27.000000000
Message: 
Patch Set 16:

(1 comment)
File Comment: /PATCHSET_LEVEL -> > 2. For the flush batch we are interested in the size-in-bytes (not size-in-num-items)

That's not strictly true, flushing 1 item at 100KB is not the same as flushing 100 items at 1KB. For magma for example you have to read 1 item vs 100, even if you write one contiguous block of 100KB at the end.

What currently happens when we fill up the last Checkpoint that the CheckpointManager has (i.e. we have 9 closed checkpoints, and one open, and cannot create more as a result)? Do we keep writing into it until overall cross-vBucket CheckpointManager quota hits the limit (i.e. does that low Checkpoint end up with way more items than the others)?
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2022-05-10 10:40:00.000000000
Message: 
Patch Set 16:

(1 comment)
File Comment: /PATCHSET_LEVEL -> > That's not strictly true, flushing 1 item at 100KB is not the same as flushing 100 items at 1KB. For magma for example you have to read 1 item vs 100, even if you write one contiguous block of 100KB at the end.

I'm not saying that 1 vs 100 items in the flush batch is the same - I'm just trying to evaluate what problems we have (already) with bytes-unbounded checkpoints vs problems that we might be hitting with item-unbounded checkpoints. As you know the first has already given us magma mem-usage issues - On the other side I'm not sure how problematic would possibly be doing more reads from disk in a single flush-batch..
I think that here we should fix the item size (let's say 1k) and compare different checkpoints with different num items, eg:
a. 1 checkpoint, 100 items
b. 2 checkpoints, 50 items

Is (a) a more performant scenario compared to (b) in magma?

[On a separate thread, more items in the single checkpoints has a positive effect on deduplication - I don't want to diverge the discussion on that here though as I think we need to focus on the problematic aspects ;) ]

> What currently happens when we fill up the last Checkpoint that the CheckpointManager has (i.e. we have 9 closed checkpoints, and one open, and cannot create more as a result)? Do we keep writing into it until overall cross-vBucket CheckpointManager quota hits the limit (i.e. does that low Checkpoint end up with way more items than the others)?

Correct, the current max-item limit isn't a hard limit in the current implementation either.
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2022-05-10 10:42:18.000000000
Message: 
Patch Set 16:

(1 comment)
File Comment: /PATCHSET_LEVEL -> > Is (a) a more performant scenario compared to (b) in magma?

I got it the other way round, question is actually whether (b) is more performant than (a) in magma - assumption is that in (b) we need 2 flusher runs for persisting everything (ie, 1 checkpoint / 1 flush)
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2022-05-10 10:50:37.000000000
Message: 
Patch Set 16:

(1 comment)
File Comment: /PATCHSET_LEVEL -> I also want to mention - Best thing here would be pushing and having a full Perf scan on it.
Note that the prod/logic change is small, the pain was on making tests not dependant from max-items. Which means, reverting the thing back with minimal test coverage would be quick.
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2022-05-10 11:00:35.000000000
Message: 
Patch Set 16: Code-Review+2

(1 comment)
File Comment: /PATCHSET_LEVEL -> > Is (a) a more performant scenario compared to (b) in magma?

Like you say, depends on de-dupe and number of flusher runs, but assuming items are disjoint and flusher run count is the same then we would pass identical flush batches to magma.

> Correct, the current max-item limit isn't a hard limit in the current implementation either.

This seems fine then, the worst cases (huge Checkpoint at end of list) should probably only be made better by this. I think that that is probably the more interesting problem/behaviour change to solve which I think you said you're still working on.
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2022-05-10 12:08:01.000000000
Message: 
Patch Set 16:

(1 comment)
File Comment: /PATCHSET_LEVEL -> > I think that that is probably the more interesting problem/behaviour change to solve which I think you said you're still working on.

Correct - this entire change-stack's purpose is to prevent that the open checkpoints grows unbounded
----------------------------------------------------------------------------------------------------------------------
