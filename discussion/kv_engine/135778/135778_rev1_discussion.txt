======================================================================================================================
DESCRIPTION:

MB-31869: Disable FlowControl for DCP Consumer

FlowControl in EPEngine aims to something that is already enforced by
the TCP Congestion Control implementation in the OS.
Plus, in memcached we already have some form of additional flow control
in Connection::executeCommandsCallback(), where we enforce a limit on
the bufferevent send-buffer (hardcoded to 40MB currently, per
DCP connection).

We believe that FlowControl in EPE is just redundant. This patch
disables it for DCP replica connections (ie, memcached DCP Consumer
only).

Change-Id: Iaac1c2083961accd530e1fc1fbb193fe4568eab6

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Paolo Cocchi
Date: 2020-09-08 13:19:58.000000000
Message: 
Uploaded patch set 1.
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2020-09-08 14:32:52.000000000
Message: 
Patch Set 1: Verified-1

Build Failed 

http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/30164/ : FAILURE

Link error: fatal error LNK1168: cannot open platform\platform_so.dll for writing ( http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/30164/ )

http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/5547/ : FAILURE

Failure of an engine_testapp test:

<pre>
Running [0013/0099]: test dcp consumer noop...../kv_engine/engines/ep/tests/ep_testsuite_dcp.cc:3647 Test failed: `" (Expected `DCP_CONTROL", got `GET_ERROR_MAP" - Unexpected last_op)
[2020-09-08T14:23:54.435Z] (110 ms) FAIL
</pre>
 ( http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/5547/ )

Failure of a CTest test [2020-09-08T14:23:54.435Z]   2/394 Test #253: ep_testsuite_dcp.value_eviction.rocksdb ..................................................................... ( http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/5547/ )

http://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/5639/ : FAILURE

Failure of an engine_testapp test:

<pre>
Running [0013/0099]: test dcp consumer noop...../kv_engine/engines/ep/tests/ep_testsuite_dcp.cc:3647 Test failed: `" (Expected `DCP_CONTROL", got `GET_ERROR_MAP" - Unexpected last_op)
[2020-09-08T14:20:49.890Z] (45 ms) FAIL
</pre>
 ( http://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/5639/ )

Failure of a CTest test [2020-09-08T14:20:49.890Z]   1/407 Test #260: ep_testsuite_dcp.value_eviction.rocksdb ..................................................................... ( http://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/5639/ )

http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/4656/ : FAILURE

Failure of an engine_testapp test:

<pre>
Running [0013/0099]: test dcp consumer noop...../kv_engine/engines/ep/tests/ep_testsuite_dcp.cc:3647 Test failed: `" (Expected `DCP_CONTROL", got `GET_ERROR_MAP" - Unexpected last_op)
[2020-09-08T14:05:13.455Z] (10 ms) FAIL
</pre>
 ( http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/4656/ )

Failure of a CTest test [2020-09-08T14:05:13.455Z]   2/404 Test #254: ep_testsuite_dcp.full_eviction.comp_active ..................................................................Child aborted ( http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/4656/ )

http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/5764/ : FAILURE

Failure of an engine_testapp test:

<pre>
Running [0013/0099]: test dcp consumer noop...../kv_engine/engines/ep/tests/ep_testsuite_dcp.cc:3647 Test failed: `" (Expected `DCP_CONTROL", got `GET_ERROR_MAP" - Unexpected last_op)
[2020-09-08T14:24:00.201Z] (447 ms) FAIL
</pre>
 ( http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/5764/ )

Failure of a CTest test [2020-09-08T14:24:00.201Z]  21/385 Test #238: ep_testsuite_dcp.value_eviction.comp_passive ................................................................Child aborted ( http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/5764/ )

http://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/master/13462/ : FAILURE

http://cv.jenkins.couchbase.com/job/kv_engine-clang_format_9/2846/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_tidy/4318/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/26120/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2020-09-09 08:22:19.000000000
Message: 
Patch Set 1:

(2 comments)
Line:14, /COMMIT_MSG -> This isn't fully accurate.. the 40MB is the size of the _userspace_ buffer we enqueue _before_ we hand it off to libevent to start sending data.. libevent will then pass as much as it can into the kernel send buffer which is of an "unknown size" (we tried to max it up to 256MB....)

Once libevent has submitted the data to the kernel we'll call back into memcached and try to send more data..

Line:18, /COMMIT_MSG -> If my memory is correct one thing we wanted with the flow control in EPE was to make sure that the noops could fly through and not be "delayed" by a full send queue which needed to be drained first, but we might not have that problem anymore ;)

----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2020-09-09 09:31:20.000000000
Message: 
Patch Set 1:

(1 comment)
Line:14, /COMMIT_MSG -> From Connection::executeCommandsCallback():

    if (isDCP() && state == State::running) {
        ..
        if (..) {
            ..
            const auto maxSendQueueSize =
                    Settings::instance().getMaxSendQueueSize();
            bool more = (getSendQueueSize() < maxSendQueueSize);
            while (more) {
                const auto ret = getBucket().getDcpIface()->step(
                        static_cast<const void*>(cookies.front().get()), this);
                switch (remapErrorCode(ret)) {
                case ENGINE_SUCCESS:
                    more = (getSendQueueSize() < maxSendQueueSize);
                    break;
                case ENGINE_EWOULDBLOCK:
                    more = false;
                    break;
        ..
            }
        }
    }

There are 2 buffers involved there:
  1. the streams ready-queues
  2. the bufferevent buffer

Conn::step() pops from (1) and copies bytes into (2), until (2) hits 'maxSendQueueSize'.
Probably we are describing the same thing, I'll improve my description.

I'll also update the info on the actual value of 'maxSendQueueSize'. You're right, 40MB is just baseline value,
Not sure if at runtime that is re-set to something else, I just see that being update in the 'config_reload_executor' path. Which means that the values stays to 40MB unless re-config happens right?

----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2020-09-09 09:39:24.000000000
Message: 
Patch Set 1:

(1 comment)
Line:14, /COMMIT_MSG -> There are 3 buffers involved:

  1. The streams ready-queue
  2. the bufferevent buffer - 40MB
  3. the kernels socket buffer - unknown size up to 256MB

It means that in _theory_ you can have 296MB of data queued up for a single connection....

----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2020-09-09 10:01:27.000000000
Message: 
Patch Set 1:

(1 comment)
Line:14, /COMMIT_MSG -> > There are 3 buffers involved..

Sure, I was considering only memcached (userspace) buffers.

But I don't see how we can have 256MB queued in the kernel socket buffer. Eg, the most unlucky scenario:
  1. bufferevent buffer just below 40MB (39.999..)
  2. We call step()
  3. at (2) we write a 20MB mutation into the buffereevent buffer
This way we may actually buffer more than 'maxSendQueueSize', but I don't see how we can reach 256MB or even less. Do we have any DCP message that can really exceed the max-item-size (20MB) ?

----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2020-09-09 10:12:08.000000000
Message: 
Patch Set 1:

(1 comment)
Line:14, /COMMIT_MSG -> Mmm actually I'm still focusing on the bufferevent (userspace) buffer in my last comment.

In the kernel we may actually buffer more (up to 256 or whatever). That may happens if libevent drains its send-buffer by writing data to the socket, but the kernel doesn't promptly drains its socket buffer.

That suggests that it may be a good to match the sizes of our bufferevent buffer limit with the actual kernel configuration?

----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2020-09-09 12:23:25.000000000
Message: 
Patch Set 1:

(1 comment)
Line:18, /COMMIT_MSG -> I've double checked with regard to Noop, current behaviour is that the Consumer noops params via DcpControl to the Producer based on cofniguration. Currently by default it takes 360 seconds of "delay" before we see a disconnection. For CB <=5.0 is was 180 secs.

What you mention can theoretically happen. But let's consider what is the behaviour today in that scenario:
 - consumer acks bytes of received DCP messages
 - producer receives bytes and drains the BufferLog
 - producer can proceed with sending more DCP messages
 - producer doesn't send anything because the send queue is full
 - at some point one peer hits the noop-interval and we disconnect anyway

So actually it seems that disabling FlowControl doesn't change the behaviour of Noops.

----------------------------------------------------------------------------------------------------------------------
Author: Dave Rigby
Date: 2020-09-10 09:13:59.000000000
MISMATCHED INLINE COMMENT
Line:18, /COMMIT_MSG -> I think it's worth including the details in your last comment here in the commit message, so there's a record alongside the code of exactly why NOOP behaviour is unchanged.
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2020-09-10 09:19:13.000000000
MISMATCHED INLINE COMMENT
Line:18, /COMMIT_MSG -> sure, doing
----------------------------------------------------------------------------------------------------------------------
