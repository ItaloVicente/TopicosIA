======================================================================================================================
DESCRIPTION:

MB-52490: Prevent that backfill-busy Producers block others

It's possible that a single Producer consumes all backfills.maxRunning
slots. That might happen in the case where a Producer is busy with many
backfills. Eg, the current backfill fixture implementation allows the
follwing:

1. backfills.maxRunning = 4096
2. Two DcpProducer connections with some active/pending backfills
   -> Prod1(active:4096, pending:100), Prod2(active:0, pending:1)
3. Prod1 completes a backfill -> Prod1(active:4095, pending:100)
4. Prod1 is rescheduled to run immediately
5. Prod2 keeps being snoozed and rescheduled to run only after its
   sleep-time (1 sec by default for the BackfillManagerTask)

The consequence is that Prod1 has much more chance to drain its pending
queue than Prod2.

This patch prevents that by increasing the likelihood that a Producer
like Prod2 gets it's backfill slot before Prod1 manages to take its next
one.

The logic change in this patch is that at step (5) Prod2 realizes that
it's being snoozed because some other Producer is consuming all the
available backfill slots. At that point Prod2 reschedules itself to run
immediately (rather than snoozing).

Change-Id: I17130e6b1e5bf66555e7fba9ea7d5682bd2be865

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Paolo Cocchi
Date: 2022-06-30 07:15:34.000000000
Message: 
Patch Set 5: Commit message was updated.
----------------------------------------------------------------------------------------------------------------------
Author: Restriction Checker
Date: 2022-06-30 07:15:44.000000000
Message: 
Patch Set 5: Well-Formed+1

Permission granted to commit: 

http://server.jenkins.couchbase.com/job/restricted-branch-check/304825/artifact/restricted.html : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Dave Rigby
Date: 2022-06-30 08:40:23.000000000
Message: 
Patch Set 5: Code-Review-1

(1 comment)
File Comment: /PATCHSET_LEVEL -> I feel like this is not really addressing the crux of the problem - which can be framed in a few different ways, but one way of looking at it is the code in BackfillManager::run():

    backfill_status_t status = manager->backfill();
    if (status == backfill_finished) {
        return false;
    } else if (status == backfill_snooze) {
        snooze(sleepTime); // sleepTime = 1s.
    }

The fundamental issue is arguably that when a BackfillManager needs to snooze (for example when the max backfills limit has been hit), he snoozes for an arbitrary amount of time (1s), then sees what the state of the world is then.

As you have identified, if Prod2 has to snooze because there's no available backfill "slots", he snoozes for 1s. In the meantime Prod1 finishes the one he was running, has a free slot, runs again, probably finishes, has a free slot, runs again... for at least the next 1s.

Then there's a race with Prod1 and Prod2's BackfillManagerTasks, whichever one wins the race will be back in the same situation again....

Ultimately I think the problem is this arbitrary snoozing / polling. Much like elsewhere (Flusher etc), we should be aiming to remove this polling. Instead we should do something like sleep /forever/, but be woken when the task can perform work - for example by maintaining a queue of "pendingBackfillMgrs", and the head of the queue is woken whenever a BackfillManagerTask finishes (assuming the queue is non-empty).

There might be other approaches which achieve the same goal, but I don't like the current approach which turns an arbitrary snooze into an arbitrary busy-wait. 



----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2022-06-30 12:57:27.000000000
Message: 
Patch Set 5:

(1 comment)
File Comment: /PATCHSET_LEVEL -> I totally agree, the change to an event-driven approach has been already discussed (eg, https://review.couchbase.org/c/kv_engine/+/148398/2..3/engines/ep/src/dcp/dcpconnmap.h#b249) and filed in multiple MBs that are set to be addressed in the master branch given the nature of the change.

This was an attempt to mitigate a relatively rare issue on Neo.
I consider that we'll not submit this at this point. There's a QE run (quite long run actually) still in progress that I won't cancel though, as we were curious with Ben to see the impact of the change under load - whether this would given the expected improvement in-primis ðŸ˜Š
----------------------------------------------------------------------------------------------------------------------
Author: Dave Rigby
Date: 2022-06-30 13:00:05.000000000
Message: 
Patch Set 5:

(1 comment)
File Comment: /PATCHSET_LEVEL -> Thanks. 

(Sorry if I've missed / forgotten about existing discussions on this issue, I only started looking at it again this morning).

It seems sensible to leave for Neo for now. Let's try to fix this "properly" on master  ðŸ˜Š
----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2024-02-26 08:25:54.000000000
MISMATCHED INLINE COMMENT
File Comment: /PATCHSET_LEVEL -> Removing myself as reviewer due to lack of updates for a few years
----------------------------------------------------------------------------------------------------------------------
