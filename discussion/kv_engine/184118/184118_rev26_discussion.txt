======================================================================================================================
DESCRIPTION:

MB-53053: Use QuotaSharingItemPager for eviction from multiple buckets

Changed the ItemPager factory method to return an instance of the
QuotaSharingItemPager which is shared between quota sharing
configurations.

Change-Id: I348c5adee260a26ea0c337c83467a6fa8cd102ed

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Vesko Karaganev
Date: 2023-01-30 14:24:23.000000000
Message: 
Uploaded patch set 26.
----------------------------------------------------------------------------------------------------------------------
Author: James H
Date: 2023-01-30 14:25:59.000000000
Message: 
Patch Set 26: Code-Review+1

(1 comment)
----------------------------------------------------------------------------------------------------------------------
Author: Vesko Karaganev
Date: 2023-01-30 14:30:27.000000000
Message: 
Patch Set 26:

(1 comment)
----------------------------------------------------------------------------------------------------------------------
Author: James H
Date: 2023-01-30 14:36:05.000000000
Message: 
Patch Set 26:

(1 comment)
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2023-01-30 15:53:30.000000000
Message: 
Patch Set 26: Verified+1

Build Successful 

https://cv.jenkins.couchbase.com/job/kv_engine-clang_tidy-master/22563/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine-windows-master/51956/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/43600/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine-clang_format_9/22435/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.libFuzzer/job/master/12287/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.linux-debug/job/master/1333/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/24614/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/25039/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/25908/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/master/33034/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.aarch64-linux/job/master/12371/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/24155/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Vesko Karaganev
Date: 2023-02-28 14:24:35.000000000
Message: 
Patch Set 26:

(1 comment)
File Comment: /PATCHSET_LEVEL -> ping for review :)
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2023-03-06 11:06:20.000000000
Message: 
Patch Set 26: Code-Review-1

(5 comments)
Line:6918, engines/ep/src/ep_engine.cc -> should probably be in configuration rather than hard coded

Line:6927, engines/ep/src/ep_engine.cc -> can this just be implemented with a magic static?

Line:33, tests/testapp_cluster/quota_sharing_paging_test.cc -> what was the result of our conversation on ptrdiff_t vs ssize_t? I can't recall but I remember talking about it before

Line:53, tests/testapp_cluster/quota_sharing_paging_test.cc -> We maybe do (to some extent) care about replicas. It's definitely easier to test without them though

Line:405, tests/testapp_cluster/quota_sharing_paging_test.cc -> Curious about the behaviour when we delete buckets. If the bucket that we were paging from contained the coldest items do we re-run the pager again and recalculate the thresholds without the hot bucket? Conversely, does it just work if the deleting bucket has the hottest items?

Can we add tests for the above?

----------------------------------------------------------------------------------------------------------------------
Author: Vesko Karaganev
Date: 2023-03-06 16:05:36.000000000
MISMATCHED INLINE COMMENT
Line:6918, engines/ep/src/ep_engine.cc -> Ack. Will do in a follow-up patch.
----------------------------------------------------------------------------------------------------------------------
Author: Vesko Karaganev
Date: 2023-03-06 16:05:36.000000000
MISMATCHED INLINE COMMENT
Line:6927, engines/ep/src/ep_engine.cc -> Done
----------------------------------------------------------------------------------------------------------------------
Author: Vesko Karaganev
Date: 2023-03-06 16:05:36.000000000
MISMATCHED INLINE COMMENT
Line:33, tests/testapp_cluster/quota_sharing_paging_test.cc -> `ssize_t` is POSIX and guaranteed to hold values from `[-1, SSIZE_MAX]` (it is not a size_t-wide signed integer type and is not in the C++ standard). However, the compilers and targets that we support do use whatever `std::make_signed_t<size_t>` as the underlying type for `ssize_t`, and we already use `ssize_t` in many places to mean `std::make_signed_t<size_t>`, so I'm fine with using `ssize_t` here for consistency.

_If_ it ever becomes a problem (`ssize_t != signed<size_t>`), things will be very obviously broken -- which makes this okay.
----------------------------------------------------------------------------------------------------------------------
Author: Vesko Karaganev
Date: 2023-03-06 16:05:36.000000000
MISMATCHED INLINE COMMENT
Line:53, tests/testapp_cluster/quota_sharing_paging_test.cc -> Updated comment, done.
----------------------------------------------------------------------------------------------------------------------
Author: Vesko Karaganev
Date: 2023-03-09 16:06:12.000000000
MISMATCHED INLINE COMMENT
Line:405, tests/testapp_cluster/quota_sharing_paging_test.cc -> The visitors yield at every vBucket, to the next bucket's vBucket (round-robin between buckets). The classic item pager's visitors are duration-capped, but that cap is only checked at every vBucket, so the quota sharing pager's visitors actually yields more often. 

The non-quota sharing pager would reschedule eviction until the low_wat is reached for value-only eviction buckets only. For full eviction, we assume that the eviction ratios are accurate enough that one pass is sufficient. 

The quota sharing pager does the same -- if _any_ bucket is value-only eviction, then we reschedule until we've reached the low_wat. If all buckets are full eviction, we only do one pass across all buckets.

> what happens when the pager run finishes, does it still check if it should run again and page things out

A: Only if we have value-only eviction buckets. 

In the context of your original question, if a bucket is deleted mid-visit, we'd only end up evicting more than is necessary, not less. And to my point above -- I don't think adding the extra complexity to check for this and stop early is a good trade off.
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2023-03-08 18:12:13.000000000
MISMATCHED INLINE COMMENT
Line:405, tests/testapp_cluster/quota_sharing_paging_test.cc -> > The bucket cannot be deleted while a visitor is running for that bucket, but it can be deleted while another bucket's visitor is running. 

Are visitor visits capped duration-wise?


I think that my question on the above was more, what happens when the pager run finishes, does it still check if it should run again and page things out?
----------------------------------------------------------------------------------------------------------------------
Author: Vesko Karaganev
Date: 2023-03-08 11:09:52.000000000
MISMATCHED INLINE COMMENT
Line:405, tests/testapp_cluster/quota_sharing_paging_test.cc -> The current behaviour is we don't attempt to recalculate the thresholds.
----------------------------------------------------------------------------------------------------------------------
Author: Vesko Karaganev
Date: 2023-03-10 16:24:18.000000000
MISMATCHED INLINE COMMENT
Line:405, tests/testapp_cluster/quota_sharing_paging_test.cc -> As discussed on Slack, the example you've given should have the hotness values swapped to make sense.

As we begin to delete BucketB, we remove it from the engine group. The cross-bucket visitor adapter will just ignore it and proceed with visiting BucketA.

If we delete a bucket and there is a cross-bucket visit running concurrently, that deleted bucket is no longer considered by that cross-bucket visit, but the visitors on other buckets continue as normal until completion. :vaultboy:
----------------------------------------------------------------------------------------------------------------------
Author: Vesko Karaganev
Date: 2023-03-06 16:05:36.000000000
MISMATCHED INLINE COMMENT
Line:405, tests/testapp_cluster/quota_sharing_paging_test.cc -> Currently, we use the `CrossBucketVisitorAdapter` to orchestrate `PagingVisitor` runs across buckets. We initialise the adapter with a map of `{bucket -> visitor}`. The bucket cannot be deleted while a visitor is running for that bucket, but it can be deleted while another bucket's visitor is running. Once the bucket is deleted, we remove the visitor from the map in the adapter. Other visitors are scheduled as normal until completed.

The only downside I can see is that we could end up evicting more items from buckets if a scheduled `PagingVisitor` run co-occurs with a bucket being deleted. It wouldn't be unfair to qualify this as undesirable, however, I don't imagine it would occur very often and it should have a very limited temporary impact on stuff. 

I don't think it is worth the extra code-complexity to allow the `CrossBucketVisitorAdapter` to cancel visitors when a bucket has been deleted (and possibly callback in the `ItemPager` to re-run).
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2023-03-09 16:22:11.000000000
MISMATCHED INLINE COMMENT
Line:405, tests/testapp_cluster/quota_sharing_paging_test.cc -> > if a bucket is deleted mid-visit, we'd only end up evicting more than is necessary, not less.

I don't think I understand this. Say we have two buckets with 1 item each. The item in BucketA has a hotness value of 10, the item in BucketB has a hotness value of 20. We then begin a pager run for which only the item in BucketB is eligible for eviction. We first start a visit to some vBucket for BucketA (which will evict nothing). We then remove the handle from the engine group for BucketB as we begin to delete it. Should the run not then evict nothing as we skip visiting BucketB as it is being deleted, BucketA has nothing evictable, and we don't recalculate thresholds as we go?

That's probably fine, we're deleting the entire HashTables for BucketB in the above example, I just want to understand how it works.
----------------------------------------------------------------------------------------------------------------------
