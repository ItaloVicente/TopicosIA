======================================================================================================================
DESCRIPTION:

MB-39422: Simplify memoryCondition() to be watermark-based

As observed when testing data load / rebalance workloads under high
DGM, when KV-Engine hits the high watermark, a large amount of work is
done on the front-end thread.

Specifically, we see that 60% of each active front-end thread is spent
inside memoryCondition(), which just decides if memory recovery should
be attempted or not (it doesn't actually recover any memory).

There looks to be a range of problems here:

1. Excessive time spent in VBucketCountVisitor::visitBucket(). 98% of
   all time in memoryCondition is spent in :visitBucket(). This is
   called to calculate the number of resident items, and if non-zero
   them the ItemPager is woken up. However visitBucket() actually
   accumulates ~50 or so stats, many of which are more expensive than
   the item counts, so there's ~48 stats which are calculated and then
   discarded.

2. Excessive calls to memoryCondition(). Every time a client operation
   fails due to not enough memory being available (i.e. at/above high
   watermark), memoryCondition is called, and performs the above
   expensive checks.
   This is very wasteful because:

   a) The ItemPager could already be running, and it cannot be
      re-scheduled again until it's finished anyway.

   b) Another client thread could already be running memoryCondition,
      so threads are repeating the same work.

3. memoryCondition is arguably over-complex in what it's trying to do.

   a) It's essentially doing two things at the same time. First it
      determines if we should return ETMPFAIL or NOMEM to the user,
      and secondly attempt to recover memory if possible.

   b) The memory recovery logic is complex / brittle. We attempt to
      predict if memory could be recovered ahead of time, with two
      possible approaches - paging out items, or closing unreferenced
      Checkpoints. However, I suspect the prediction isn't always
      correct in determining if any more memory can be freed, as it
      relies on indirect metrics like number of items resident, which
      could already be zero, and memory is in use elsewhere.

Attempt to address all the above issues by simplifying what
memoryCondition does - replace the decision on when to wake the
ItemPager with a much simpler, cheaper, direct policy - base the
memory freeing policy on memory watermarks:

- If bucket mem_used between mutation_mem_threshold and bucket_quota
  (i.e. 93% and 100% of bucket) then attempt to wake ItemPager &
  CheckpointRemover, then return ETMPFAIL

- If bucket mem_used over bucket_quota then attempt to wake ItemPager
  & CheckpointRemover, then return ENOMEM.

The rationale here is that to get to mutation_mem_threshold (93% of
memory bucket quota) then the bucket must already be under some duress
- either a rapid burst of mutations taking memory significantly over
the high watermark (85%), or one or more slow Checkpoint Cursors
(Flusher, DCP) causing memory usage to be very high. Either way, we
should attempt to trigger all memory freeing tasks to bring usage
down.

This addresses (1) as we no longer call
VBucketCountVisitor::visitBucket(). It indirectly address (2) by
making memoryCondition() cheap to call - both attemptToFreeMemory()
and wakeUpCheckpointRemover() check atomic flags and only if not
already scheduled will attempt to schedule those tasks. It addresses
(3) by adding a much simpler policy.

With this change the CPU consumed by each front-end thread inside
memoryCondition() is down to ~1% (from 60%).

Change-Id: I80e2e6be0323edb2e51c08da021155705cbb8e38

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Dave Rigby
Date: 2020-07-07 10:53:57.000000000
Message: 
Patch Set 6: Patch Set 5 was rebased
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2020-07-07 11:38:32.000000000
Message: 
Patch Set 6: Verified+1

Build Successful 

http://cv.jenkins.couchbase.com/job/kv_engine-clang_format_9/1744/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/25061/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/29041/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_tidy/3237/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.macos/job/master/3698/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/4765/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.linux/job/master/4663/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/master/4589/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/master/12484/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2020-07-07 16:03:24.000000000
Message: 
Patch Set 6: Code-Review-1

(1 comment)
Line:2692, engines/ep/src/ep_engine.cc -> should be tmp_oom for full eviction still?

----------------------------------------------------------------------------------------------------------------------
Author: Dave Rigby
Date: 2020-07-07 16:13:52.000000000
Message: 
Patch Set 6:

(1 comment)
Line:2692, engines/ep/src/ep_engine.cc -> No, I don't think it should - given it was somehwhat of a lie that full-eviction is somehow guaranteed to never hard-OOM.

For example, we have encountered plenty of situations where we cannot free memory, but claiming this is just a tempOOM if we are north of the wall^Wbucket quota feels wrong.

----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2020-07-07 16:22:50.000000000
Message: 
Patch Set 6: Code-Review+2
----------------------------------------------------------------------------------------------------------------------
