======================================================================================================================
DESCRIPTION:

MB-41514: Set reader priority to same as writers

Whilst we de-prioritize writers to prevent them from impacting the
front end too badly, we currently do nothing for readers. This
effectively makes them higher priority than writers which is not always
desirable. Both are disk operations and shouldn't be prioritized over
one another.

Change-Id: I801607d2358ce7c151ee8f6de20aadca957999fb

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Ben Huddleston
Date: 2020-10-30 08:46:40.000000000
Message: 
Uploaded patch set 1.
----------------------------------------------------------------------------------------------------------------------
Author: Dave Rigby
Date: 2020-10-30 09:12:57.000000000
Message: 
Patch Set 1:

Note: I suspect this could be a bit tricky to get well balanced. 

Arguably some reader tasks (specifically BGfetches) _should_ have higher priority than (some) writes, given that the client requests' latency (of a non-resident item) is dependant on how quickly we read from disk. As such, on a system with contention on which thread kernel should run (which is the only time the priorities make a difference) you would actually prefer a BGFetch task to be scheduled before a Flusher task flushing regular mutations). Similary, BGFetcher tasks should be prioritised ahead of compaction.

However, _if_ there are SyncWrites in play then they have similar properties to BGFetches - the client request is dependant on them completing.

Ideally we want something like high-priority IO threads _and_ low-priority IO threads (not necessarily the current reader/writer/auxio split:

- High priority IO threads used for SyncWrite Flushes, BGFetches
- Low priority IO threads used for normal Flushes, compaction. Backfill also probably belongs here.

That's a direction I want to move to with the FollyExecutorPool (although strictly speaking it's not tied to that directly) - we arguably have far too any thread pools right now (6 if I'm counting correctly, and that's excluding Magma!)
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2020-10-30 09:40:00.000000000
Message: 
Patch Set 1:

> Patch Set 1:
> 
> Note: I suspect this could be a bit tricky to get well balanced. 
> 
> Arguably some reader tasks (specifically BGfetches) _should_ have higher priority than (some) writes, given that the client requests' latency (of a non-resident item) is dependant on how quickly we read from disk. As such, on a system with contention on which thread kernel should run (which is the only time the priorities make a difference) you would actually prefer a BGFetch task to be scheduled before a Flusher task flushing regular mutations). Similary, BGFetcher tasks should be prioritised ahead of compaction.
> 
> However, _if_ there are SyncWrites in play then they have similar properties to BGFetches - the client request is dependant on them completing.
> 
> Ideally we want something like high-priority IO threads _and_ low-priority IO threads (not necessarily the current reader/writer/auxio split:
> 
> - High priority IO threads used for SyncWrite Flushes, BGFetches
> - Low priority IO threads used for normal Flushes, compaction. Backfill also probably belongs here.
> 
> That's a direction I want to move to with the FollyExecutorPool (although strictly speaking it's not tied to that directly) - we arguably have far too any thread pools right now (6 if I'm counting correctly, and that's excluding Magma!)

Far too many pools atm indeed! I agree with what you said, this is just a forward port of a change you suggested wwhen I was looking at bg fetcher scheduling so I could test this too if the results are not ideal
----------------------------------------------------------------------------------------------------------------------
