======================================================================================================================
DESCRIPTION:

MB-36572: keep couchstore files open across writes

Currently CouchKVStore opens and closed the underlying couchstore file
on every call to saveDocs(). Profiling has highlighted that the
underlying OS close() call (and to a lesser extent the open() call)
are costly - on Linux with XFS there appears to be non-trivial work to
release various allocated resources - close() takes around 7% of the
total flusher runtime.

To improve this, keep the couchstore Db instance for each vBucket open
across saveDocs() calls. This increases KV-Engine's FD usage
(potentially 1024 more FDs open at once than before), but should
reduce the costs associated with open/close.

Results
-------

* Concurrent SyncWrite workload (pillowfight, 2000 threads)

This change reduces the average persistTo latency from 748ms to
614ms - 1.2x faster persistence.

Running on mancouch, Samsung PM863 SSD, XFS, with a 2 node
cluster_run, Assuming already populated with 3.2M items:

    cbc-pillowfight -U couchbase://127.0.0.1:12000/default \
        -u Administrator -P asdasd --batch-size=1 --num-items=3200000 \
        --min-size=512 --max-size=512 --random-body --set-pct=100 \
        --no-population --durability=persist_to_majority --num-threads=2000 \
        --sequential --num-cycles=100

Baseline (before this patch):

 sync_write_commit_persist_to_majority (100000 total)
       0us - 3454us : (  0.0010%)     1
    3454us -  327ms : ( 10.0770%) 10076 ███████████████████▏
     327ms -  557ms : ( 21.7620%) 11685 ██████████████████████▎
     557ms -  688ms : ( 32.1070%) 10345 ███████████████████▊
     688ms -  753ms : ( 42.5240%) 10417 ███████████████████▉
     753ms -  786ms : ( 50.0440%)  7520 ██████████████▎
     786ms -  819ms : ( 56.3020%)  6258 ███████████▉
     819ms -  851ms : ( 62.6690%)  6367 ████████████▏
     851ms -  884ms : ( 68.3760%)  5707 ██████████▉
     884ms -  917ms : ( 73.9490%)  5573 ██████████▋
     917ms -  950ms : ( 79.5080%)  5559 ██████████▌
     950ms -  983ms : ( 84.4990%)  4991 █████████▌
     983ms - 1015ms : ( 88.9030%)  4404 ████████▍
    1015ms - 1048ms : ( 91.4420%)  2539 ████▊
    1048ms - 1114ms : ( 94.6060%)  3164 ██████
    1114ms - 1179ms : ( 96.9820%)  2376 ████▌
    1179ms - 1245ms : ( 98.7690%)  1787 ███▍
    1245ms - 1310ms : ( 99.2830%)   514 ▉
    1310ms - 1376ms : ( 99.4930%)   210 ▍
    1376ms - 1441ms : ( 99.5610%)    68 ▏
    1441ms - 1507ms : ( 99.6960%)   135 ▎
    1507ms - 1572ms : ( 99.8230%)   127 ▏
    1572ms - 1638ms : ( 99.9570%)   134 ▎
    1638ms - 1703ms : (100.0000%)    43
    Avg             : (  748ms)

After this patch:

 sync_write_commit_persist_to_majority (100000 total)
       0us - 3198us : (  0.0010%)     1
    3198us -  262ms : ( 11.0100%) 11009 █████████████████████
     262ms -  360ms : ( 21.5290%) 10519 ████████████████████
     360ms -  458ms : ( 31.0540%)  9525 ██████████████████▏
     458ms -  557ms : ( 40.0470%)  8993 █████████████████▏
     557ms -  655ms : ( 50.8740%) 10827 ████████████████████▋
     655ms -  688ms : ( 55.3550%)  4481 ████████▌
     688ms -  720ms : ( 60.4080%)  5053 █████████▋
     720ms -  753ms : ( 65.2010%)  4793 █████████▏
     753ms -  786ms : ( 70.4710%)  5270 ██████████
     786ms -  819ms : ( 76.1430%)  5672 ██████████▊
     819ms -  851ms : ( 81.5420%)  5399 ██████████▎
     851ms -  884ms : ( 84.7680%)  3226 ██████▏
     884ms -  917ms : ( 87.5950%)  2827 █████▍
     917ms -  950ms : ( 90.8640%)  3269 ██████▏
     950ms -  983ms : ( 93.3300%)  2466 ████▋
     983ms - 1015ms : ( 95.3830%)  2053 ███▉
    1015ms - 1048ms : ( 96.7730%)  1390 ██▋
    1048ms - 1114ms : ( 98.6620%)  1889 ███▌
    1114ms - 1179ms : ( 99.6900%)  1028 █▉
    1179ms - 1245ms : ( 99.9680%)   278 ▌
    1245ms - 1310ms : (100.0000%)    32
    Avg             : (  614ms)

* For a single-threaded SyncWrite workload (pillowfight, 1 thread)

This changes gives a slight (1.05x) reduction in persistTo latency. I
suspect this is due to the fact that much smaller commit batch sizes
are seen, and hence each couchstore file increases by a much smaller
size than the concurrent workload above.

    cbc-pillowfight -U couchbase://127.0.0.1:12000/default \
        -u Administrator -P asdasd --batch-size=1 --num-items=3200000 \
        --min-size=512 --max-size=512 --random-body --set-pct=100 \
        --no-population --durability=persist_to_majority --num-cycles=10000

Baseline (before this patch):

 sync_write_commit_persist_to_majority (5204 total)
       0us - 2302us : (  0.0576%)   3
    2302us - 3710us : ( 14.4696%) 750 ███████████████████████████▊
    3710us - 3838us : ( 20.5227%) 315 ███████████▋
    3838us - 4094us : ( 34.8002%) 743 ███████████████████████████▌
    4094us - 4350us : ( 48.9816%) 738 ███████████████████████████▎
    4350us - 4606us : ( 61.2798%) 640 ███████████████████████▋
    4606us - 4862us : ( 71.8101%) 548 ████████████████████▎
    4862us - 5118us : ( 79.9193%) 422 ███████████████▋
    5118us - 5374us : ( 86.0300%) 318 ███████████▊
    5374us - 5630us : ( 90.5073%) 233 ████████▋
    5630us - 5886us : ( 93.8317%) 173 ██████▍
    5886us - 6142us : ( 95.5611%)  90 ███▎
    6142us - 6398us : ( 96.6756%)  58 ██▏
    6398us - 6654us : ( 97.4058%)  38 █▍
    6654us - 6910us : ( 97.8286%)  22 ▊
    6910us - 7422us : ( 98.2129%)  20 ▋
    7422us - 7934us : ( 98.5012%)  15 ▌
    7934us - 8702us : ( 98.6357%)   7 ▎
    8702us -   11ms : ( 98.7510%)   6 ▏
      11ms -   31ms : ( 99.1545%)  21 ▊
      31ms -   32ms : ( 99.5965%)  23 ▊
      32ms -   34ms : ( 99.9616%)  19 ▋
      34ms -   36ms : ( 99.9808%)   1
      36ms -   40ms : (100.0000%)   1
    Avg             : ( 4822us)

After this patch:

 sync_write_commit_persist_to_majority (5204 total)
       0us - 1982us : (  0.0192%)   1
    1982us - 3454us : ( 12.8939%) 670 ████████████████████████▊
    3454us - 3710us : ( 24.9424%) 627 ███████████████████████▎
    3710us - 3838us : ( 32.4942%) 393 ██████████████▌
    3838us - 4094us : ( 45.6764%) 686 █████████████████████████▍
    4094us - 4350us : ( 59.4927%) 719 ██████████████████████████▋
    4350us - 4606us : ( 71.2721%) 613 ██████████████████████▋
    4606us - 4862us : ( 79.5926%) 433 ████████████████
    4862us - 5118us : ( 86.0876%) 338 ████████████▌
    5118us - 5374us : ( 90.4112%) 225 ████████▎
    5374us - 5630us : ( 93.2552%) 148 █████▍
    5630us - 5886us : ( 95.2921%) 106 ███▉
    5886us - 6142us : ( 96.4643%)  61 ██▎
    6142us - 6398us : ( 97.2713%)  42 █▌
    6398us - 6654us : ( 97.7902%)  27 █
    6654us - 6910us : ( 98.1360%)  18 ▋
    6910us - 7422us : ( 98.5972%)  24 ▉
    7422us - 8702us : ( 98.7510%)   8 ▎
    8702us -   30ms : ( 98.9816%)  12 ▍
      30ms -   31ms : ( 99.2314%)  13 ▍
      31ms -   32ms : ( 99.6925%)  24 ▉
      32ms -   34ms : ( 99.9616%)  14 ▌
      34ms -   36ms : ( 99.9808%)   1
      36ms -   43ms : (100.0000%)   1
    Avg             : ( 4590us)

TODO: Interlock compaction more strictly. We _should_ be safe at the
moment as external locking is used to obtain mutual exclusion between
flushing and compaction, however that seems error-prone given we
should be avble to go that inside CouchKVStore.

Change-Id: Iec00f19935a96213d121dfcdfe46363581e63013

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Dave Rigby
Date: 2019-10-23 13:28:12.000000000
Message: 
Uploaded patch set 7: Patch Set 6 was rebased.
----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2019-10-23 14:59:07.000000000
Message: 
Patch Set 7:

(1 comment)
Line:18, /COMMIT_MSG -> I assume this is per bucket? so in _theory_ it could be 30k? We set aside a 1k "buffer" of file descriptors for "temporary internal use" when we calculate the maximum number of connections the system may accept.... This means that the likelihood of us running out of filedescriptors today is relatively low.. this change would significantly increase the odds of failing to get a file descriptor. I'm not afraid if that happens when we try to accept a client as I know that we properly handle that situation, but I'm not confident that we properly (gracefully) handle all other cases where we try to create a file descriptor for some reason.....

----------------------------------------------------------------------------------------------------------------------
Author: Dave Rigby
Date: 2019-10-23 15:01:47.000000000
Message: 
Patch Set 7:

(1 comment)
Line:18, /COMMIT_MSG -> Yes, it would be per-Bucket - assuming a single-node "cluster" with 30 buckets (FDs are only opened for the vBuckets which that node has a copy of).

You raise a valid point that we need to be careful with the FD use; and I have a follow-up patch in development which attempts to be more robust in this area - for example closing couchstore DBs when they go to dead; and handling the case where FD open fails by closing all open ones for that bucket.

However, I wanted to get this initial patch with the main functionality in initially.

----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2019-10-23 15:07:45.000000000
Message: 
Patch Set 7: Verified-1

Build Failed 

http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/24332/ : FAILURE

Timeout of a CTest test  52/191 Test  #53: ep_testsuite_xdcr.value_eviction.comp_passive ................................} ( http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/24332/ )

Failure of a GoogleTest test:

DCPTest.MB30189_addStats (70 ms) ( http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/24332/ )

Failure of a CTest test  ( http://cv.jenkins.couchbase.com/job/kv_engine-windows-master/24332/ )

http://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/master/7922/ : FAILURE

http://cv.jenkins.couchbase.com/job/kv_engine-threadsanitizer-master/15260/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-linux-master/23254/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-master/20729/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-linux-master-CE/4900/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv-engine-cv-perf/13969/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine-clang_format/21949/ : SUCCESS

http://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/master/52/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2019-10-23 18:41:48.000000000
Message: 
Patch Set 7:

(1 comment)
Line:18, /COMMIT_MSG -> (Or you can have a 3 node cluster with two replicas and 30 buckets ;))... anyway. the thing is that we'd need to be more robust across the entire system when we start operating outside the 1k buffer we've set aside... we call "tmpfile" a number of places in our system.. do all of them handle gracefully that they fail to open the file (not only catch the error, but can recover from it).. we use opendir.. does that work? etc.. 

It does feel a bit risky; I don't know how well we test running our system with max number of clients and buckets..

----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2019-10-24 05:44:22.000000000
Message: 
Patch Set 7:

(1 comment)
Line:18, /COMMIT_MSG -> I know at least the following cases we need to verify (but there is probably more):

1) we want to rotate to the next log file.. does spdlogger open first and continue to use the old one if it fails to open a new one? or do we end up running without logging?

2) If ns_server asks us to reload the configuration file and we fail to do so and return an error back to ns_server (or even worse it throws an exception which we throw and it is caught in our run-loop and we close the connection). THe latter will cause ns_server to kill us, but I don't know how it deals with the first one.

3) Adding a new user to the system - we need to reload the database file.. same as 2.

(we _could_ of course increase the reserved pool (and bump it up with 1k per bucket created). As long as the number the OS allows us to use is > configured max number of clients we're not entering new and unknown waters)... 

we should of course investigate and fix the real problem, but thats a separate issue.

----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2019-10-24 07:36:25.000000000
Message: 
Patch Set 7:

(1 comment)
Line:18, /COMMIT_MSG -> I just looked in the code and:

 > 
 > 2) If ns_server asks us to reload the configuration file and we
 > fail to do so

We catch the exception and don't tell ns_server about the error and we'll be running with the old configuration. This could be a problem if for instance the new configuration tried to change the interface descriptions (using new SSL certificates, add / remove ports (only SSL or allow all etc)). Filed MB-36630 to track that

 > 
 > 3) Adding a new user to the system - we need to reload the database
 > file.. same as 2.
 
We catch the error and return the error back to ns_server.. I don't know if ns_server properly handle this (need to verify that). We currently disconnect ns_server if we fail to start the thread to reload the database; we should probably return ENGINE_TMPFAIL to ns_server and have it try again). Filed MB-36631 to track fixing the disconnect. We need to test and verify that ns_server deals with the error failing to reload the database).

----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2019-10-24 07:40:22.000000000
Message: 
Patch Set 7: Code-Review+1

(1 comment)

Need to make sure that QE tests with running out of file descriptors...
Line:18, /COMMIT_MSG -> > 1) we want to rotate to the next log file.. does spdlogger open
 > first and continue to use the old one if it fails to open a new
 > one? or do we end up running without logging?
 > 

Looks safe.. we'll continue to use the same file if open fails

----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2019-10-24 11:37:35.000000000
Message: 
Patch Set 7: Code-Review+2

(3 comments)

We need to make sure that QE really tests the edge case when running out of file descriptors :S

Do we want to add a chicken-bit to always do close on unlock?
Line:1266, engines/ep/src/couch-kvstore/couch-kvstore.cc -> nit: auto

Line:2250, engines/ep/src/couch-kvstore/couch-kvstore.cc -> nit: auto

Line:458, engines/ep/src/couch-kvstore/couch-kvstore.h -> (I personally prefer to use protected, as that opens up for creating a subclass to mock around with the object in tests)

----------------------------------------------------------------------------------------------------------------------
Author: Trond Norbye
Date: 2019-10-24 15:31:02.000000000
Message: 
Patch Set 7:

> (3 comments)
 > 
 > We need to make sure that QE really tests the edge case when
 > running out of file descriptors :S
 > 
 > Do we want to add a chicken-bit to always do close on unlock?

And we should add logic in the memcached core to bump the reserved pool for each bucket we create (and recalculate the limits)
----------------------------------------------------------------------------------------------------------------------
Author: Jim Walker
Date: 2019-10-30 13:09:49.000000000
Message: 
Patch Set 7: Code-Review+2
----------------------------------------------------------------------------------------------------------------------
Author: Dave Rigby
Date: 2020-05-05 14:34:58.000000000
Message: 
Assignee added: Ben Huddleston <ben.huddleston@couchbase.com>
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2020-05-22 14:43:24.000000000
Message: 
Patch Set 7:

Just making a note for future me.

I rebased this but end up with some unit tests failing in teardowns. They fail because we provide CouchKVStore with a mock FileOps object for injecting errors and we destroy this before we destroy the engine (and hence CouchKVStore objects). This causes bad access errors as we need the file ops object to close the file (which we now do in the dtor of CouchKVStore).
----------------------------------------------------------------------------------------------------------------------
