======================================================================================================================
DESCRIPTION:

MB-54591: ActiveStream: Avoid lost wakeup due to race on itemsReady

+Summary+

There exists a race in ActiveStream between a notifying that a new
mutation available, and processing the previous mutation(s). If the
notification occurs just before ActiveStream sets a flag indicating
that there's currently no more items to process, then the notification
is lost and the DCP stream is not notified of this new mutation when
it should be.

This results in the DCP stream being behind by that seqno, until the
the next mutation on the vBucket occurs with will re-notify the stream
and cause it to process both.

+Impact+

When it occurs, this bug introduces additional latency between a
mutation occurring on the KV node, and it getting passed to the
affected DCP client. For DCP connections which require a "current" set
of seqnos to proceed - e.g. GSI when using request_plus queries, this
can manifest as those operations hanging.

Note the latency added is bounded by the mutation rate on the vBucket
- the next mutation is guaranteed to wake up the stream (as it is now
idle and there's no possible race). For workloads with moderate
mutation rates (e.g. 1000 mutations per second per Bucket), the
additional latency on average should be 1s. However for workloads with
very low mutation rates (<100 per second) then on average the latency
would be 10+ seconds.

For workloads which are intermittent and idle for extended periods,
the added latency could be large - as long as it takes for that
vBucket to be modified again.

+Details+

The race is between the itemsReady flag being tested in
notifyStreamReady, and clearing itemsReady when there are no more
items available for the stream in ActiveStream::next(). Flag is tested
here:

    void ActiveStream::notifyStreamReady(bool force, DcpProducer* producer, uint64_t seqno) {
        bool inverse = false;
        if (force || itemsReady.compare_exchange_strong(inverse, true)) {

            if (seqno) {
                itemsReadySeqnosForNotify.lock()->push_back(seqno);
            }
    ...

And it is set/reset here:

    std::unique_ptr<DcpResponse> ActiveStream::next(DcpProducer& producer) {
        std::unique_ptr<DcpResponse> response;
        switch (state_.load()) {
        ...
        case StreamState::InMemory:
            response = inMemoryPhase(producer);
            break;
        ...
        }

       itemsReady.store(response ? true : false);
       return response;
   }

We can lose a notification (from ActiveStream::notifyStreamReady) from
one front-end thread (performing a set) if the notification occurs
while the DCP thread is executing ActiveStream::next() and has already
called inMemoryPhase() and found the CheckpointManger empty - but
*before* we clear itemsReady at the end of the function.

To fix the lost wakeup we clear itemsReady _before_ we fetch the next
response - and only set it back to true (inhibiting notifies) if we
find we have at least one more item.

This potentially swaps lost wakeups for unnecessary wakeups (i.e. if a
notifyStreamReady() occurs in ActiveStream::next() between clearing
itemsReady and re-setting it at the end) - but correctness >
performance.

Change-Id: I3ed9ff1abfe654b8ced4e9c8d084bd0ddaa668ee

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Dave Rigby
Date: 2022-11-21 10:51:31.000000000
Message: 
Uploaded patch set 3.
----------------------------------------------------------------------------------------------------------------------
Author: Restriction Checker
Date: 2022-11-21 10:51:44.000000000
Message: 
Patch Set 3: Well-Formed+1

Permission granted to commit: 

https://server.jenkins.couchbase.com/job/restricted-branch-check/325698/artifact/restricted.html : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Ben Huddleston
Date: 2022-11-21 10:58:31.000000000
Message: 
Patch Set 3: Code-Review+2
----------------------------------------------------------------------------------------------------------------------
Author: Paolo Cocchi
Date: 2022-11-21 11:00:02.000000000
Message: 
Patch Set 3: Code-Review+2
----------------------------------------------------------------------------------------------------------------------
Author: Build Bot
Date: 2022-11-21 12:02:44.000000000
Message: 
Patch Set 3: Verified-1

Build Failed 

https://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/mad-hatter/1965/ : FAILURE

ThreadSanitizer issue: lock-order-inversion (potential deadlock) (/home/couchbase/jenkins/workspace/ngine.threadsanitizer_mad-hatter/install/lib/libtsan.so.0+0x5ea8d) in AnnotateRWLockAcquired  ( https://cv.jenkins.couchbase.com/job/kv_engine.threadsanitizer/job/mad-hatter/1965/ )

https://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/mad-hatter/2170/ : FAILURE

AddressSanitizer issue: heap-use-after-free ../kv_engine/programs/engine_testapp/mock_server.cc:77 in cookie_to_mock_object(void const*) ( https://cv.jenkins.couchbase.com/job/kv_engine.ASan-UBSan/job/mad-hatter/2170/ )

https://cv.jenkins.couchbase.com/job/kv_engine-windows-madhatter/2333/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine-clang_analyzer-mad-hatter/2086/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine-clang_format/26702/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.libFuzzer/job/mad-hatter/236/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.linux-CE/job/mad-hatter/2184/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.linux/job/mad-hatter/2184/ : SUCCESS

https://cv.jenkins.couchbase.com/job/kv_engine.macos/job/mad-hatter/1535/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
