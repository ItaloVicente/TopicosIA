======================================================================================================================
DESCRIPTION:

High throughput doc loader.
1. A simple loader that loads/updates/validates docs into/from couchbase server with high throughput
2. The script takes following parameters:
  --help                Print help section
  --spec SPEC, -U SPEC  Cluster connections string. [Default=couchbase://localhost]
  --bucket BUCKET, -b BUCKET
                        Bucket to connect to
  --password PASSWORD, -p PASSWORD
                        User password
  --user USER, -u USER  Username
  --batch_size BATCH_SIZE, -B BATCH_SIZE
                        Batch size of eatch inserts
  --prefix PREFIX, -k PREFIX
                        Key Prefix
  --timeout TIMEOUT, -t TIMEOUT
                        KV Operation Timeout
  --count COUNT, -c COUNT
                        Number of documents in the bucket already
  --start_document START_DOCUMENT
                        Starting document count to start updating from
  --passes PASSES, -P PASSES
                        Number of mutation cycles to perform per document (including create)
  --update_counter UPDATE_COUNTER
                        Starting update counter to start updating from
  --cb_version CB_VERSION
                        Current version of the couchbase cluster
  --size SIZE           Size of the document to be inserted, in bytes
  --validation VALIDATION, -v VALIDATION
                        Validate the documents. 0=No validation, 1=Validate at end, 2=Validate after each pass, 3=Perform only validation and no mutation. [Default=0]
  --replicate_to REPLICATE_TO
                        Perform durability checking on this many replicas for presence in memory
  --ttl TTL             Set expiry timer for documents
  --delete              Delete documents from bucket
  --num_delete NUM_DELETE
                        Number of documents to delete
  --deleted             Was delete of documents run before validation
  --deleted_items DELETED_ITEMS
                        Number of documents that were deleted
  --validate_expired    Validate if documents have expired
  --rate_limit RATE_LIMIT, -r RATE_LIMIT
                        Set operations per second limit

3. The tool can create documents and update them based on the count parameter.
4. For pure creates, give passes=1. For both create and updates, give passes=(num of mutations required + 1).
5. For pure updates, include update_counter = (value of update in document + 1).
6. For deletes, include delete and num_delete parameters.
7. The tool can also validate the data loaded. Include validation parameter for validation.
8. For no validation, give validation=0
   For validation at the end of the load generation, give validation=1
   For validation after each pass of mutation, give validation=2
   For only validation and no mutations, give validation=3
   With validation=3, pass -update_counter=value of update in a document (and --deleted and --deleted_items=docs_deleted)
9. The rate of mutations can be limited using rate_limit (to the best of ability)
10. Expiry of documents can be set using ttl parameter
    Validation of expiry of documents can be done by using --validate_expired
11. Singe instance can perform inserts, updates, deletes and validation based on the input to the script.

ex:
For creates of 1000000 document with 20K ops/sec:
python thanosied.py -U 10.112.176.101 -b default -p password -u Administrator -B 200 -c 1000000 -P 1 -v 0 -r 20000
For creates 1000000 document and update them 2 times with 20K ops/sec:
python thanosied.py -U 10.112.176.101 -b default -p password -u Administrator -B 200 -c 1000000 -P 3 -v 0 -r 20000
For updating the above 1000000 documents 2 more times with 20K ops/sec:
python thanosied.py -U 10.112.176.101 -b default -p password -u Administrator -B 200 -c 1000000 -P 2 -v 0 -r 20000 --update_counter 3
For creates of 1000000 documents and update them 2 times and delete 10000 document in the end with 20K ops/sec:
python thanosied.py -U 10.112.176.101 -b default -p password -u Administrator -B 200 -c 1000000 -P 3 -v 0 -r 20000 --delete --num_delete 10000
For creates 1000000 document and update them 2 times with 20K ops/sec and validation at the end:
python thanosied.py -U 10.112.176.101 -b default -p password -u Administrator -B 200 -c 1000000 -P 3 -v 1 -r 20000
For creates of 1000000 documents and update them 2 times and delete 10000 document in the end with 20K ops/sec and validation at end:
python thanosied.py -U 10.112.176.101 -b default -p password -u Administrator -B 200 -c 1000000 -P 3 -v 1 -r 20000 --delete --num_delete 10000
For validation without any mutation of 1000000 documents which were updated 3 times:
python thanosied.py -U 10.112.176.101 -b default -p password -u Administrator -B 200 -c 1000000 -P 3 -v 3 -r 20000 --update_counter 3

Latest patch contains following changes:
1. Review comments and suggestions from previous patches.
2. Redesigned the script slightly to have all operations in a single run
3. Included rate-limit.

Change-Id: I5e883b9ddc659cd3068c2da91e559d6c277dd460

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Bharath G P
Date: 2018-04-30 21:39:46.000000000
Message: 
Uploaded patch set 6.
----------------------------------------------------------------------------------------------------------------------
Author: Pavel Paulau
Date: 2018-05-01 04:24:09.000000000
Message: 
Patch Set 6:

(13 comments)

I am testing this tool but so far it looks very promising. I will post more comments as I get more familiar with the implementation.
Line:48, scripts/thanosied.py -> Perhaps this should belong to the connection string.

Line:80, scripts/thanosied.py -> nit: Why not len()?

Line:115, scripts/thanosied.py -> nit: Why 5?

Line:136, scripts/thanosied.py -> Are you sure this is necessary? In our perf tests we never needed to customize bucket constractor.

Line:260, scripts/thanosied.py -> This is a little bit confusing. What kind of error are you trying to handle?

Line:380, scripts/thanosied.py -> This is a process pool, right?

Line:382, scripts/thanosied.py -> It doesn't look like other tasks can be submitted to the pool.

Line:489, scripts/thanosied.py -> I think argparse already coverts this to int, doesn't it?

Line:492, scripts/thanosied.py -> nit: There seem to be a lot of integers converted to integers. Not a big a deal but quite a distraction. This comment applies to many other lines in this module.

Line:493, scripts/thanosied.py -> Why 25?

Line:498, scripts/thanosied.py -> nit: for i in range(num_workers):

Line:500, scripts/thanosied.py -> Out of curiosity, why do you need a deep copy?

Line:531, scripts/thanosied.py -> You probably want to daemonize the child processes by setting worker.daemon to True. Otherwise workers keep running even the parent processes are interrupted.

Alternatively, you can add exception handlers to guarantee a proper shutdown.

----------------------------------------------------------------------------------------------------------------------
Author: David Haikney
Date: 2018-05-01 11:20:06.000000000
Message: 
Patch Set 6:

(8 comments)

Added some comments - major one is the confusion of multiprocessing and threads. If I get the chance I'll try and post a draft with some of the suggested improvements.
Line:22, scripts/thanosied.py -> Think we only need three states:
0 - no validation
1 - End only
2 - Every pass
The "no mutations" can be accomplished by using validation =1 and passes=0

Line:75, scripts/thanosied.py -> Doesn't appear to be wired up anywhere or have I missed that?

Line:163, scripts/thanosied.py -> Almost the same function as above - suggests the operation type should be a property of the batch and we can remove the duplication.

Line:189, scripts/thanosied.py -> This code is duplicated in a  few places - suggest breaking out into a get_key function

Line:245, scripts/thanosied.py -> This needs to be able to handle timeouts and not treat it as a missing key / data loss. Suggest a limited number of retries (we can't make the script hang indefinitely) and recording the fact that we were unable to retrieve a given key.

Line:409, scripts/thanosied.py -> As above, there's very little difference here between the single upset pass and this. Suggest we find a way of folding the two functions together

Line:421, scripts/thanosied.py -> args should be static - if we need to update the state this should be written to Documentgenerator

Line:531, scripts/thanosied.py -> The combination of Threads and multi-processing adds complexity and breaks running locally on a mac - we need to be able to do this to run locally against cluster_run etc.  Can we look at the performance with / without the combination of multi-processing and threads. e.g. with Threads alone I was able to derive 25K writes / sec.

----------------------------------------------------------------------------------------------------------------------
Author: Pavel Paulau
Date: 2018-05-01 19:57:47.000000000
Message: 
Patch Set 6:

(2 comments)

I briefly checked the validation step and it seems to be working as expected.
Line:37, scripts/thanosied.py -> I don't quite understand why each batch should take at least  a second. That assumption significantly reduces the maximum rate of operations especially when small (<100) batches are used.

I think this sleep time should be a function of the number of workers and the target rate of operations.

Line:297, scripts/thanosied.py -> Why do you need this and other similar wrappers?

----------------------------------------------------------------------------------------------------------------------
Author: Bharath G P
Date: 2018-05-02 12:12:38.000000000
Message: 
Patch Set 6:

(23 comments)

Thank you David, Ellis and Pavel for all the comments and help on the tool. I have fixed/answered most of the comments. I will send another patch with the latest fixes soon.
Line:22, scripts/thanosied.py -> Good point. I will make the change in the next patch. I had included a separate state to keep a separate state for just validation.

Line:37, scripts/thanosied.py -> I agree that this reduces the maximum capability of the script. But to keep the calculations simple, I adopted this way to limit each thread to run atleast 1 sec so that we can have rate limit in the tool. If you can help me out with the formula that you are talking about, I can include it here in the next iteration.

Line:48, scripts/thanosied.py -> For 5.0 and later version, we recommend using Cluster object to create the connections (atleast in our official help page.). Hence the need to support separate bucket param.

Line:75, scripts/thanosied.py -> This is currently being used in calculation of number of workers to create. (line 492)

Line:80, scripts/thanosied.py -> Will fix in next patch

Line:115, scripts/thanosied.py -> From my experimental testing of the tool and previous iterations, I found 5 threads gave good throughput in general. We can include a separate param for the user to control the threads, but for now kept it simple and limiting it to 5 threads per instance of the class.

Line:136, scripts/thanosied.py -> According to our recommendation on our help page, we need to use the cluster object for connection. Also have had few issues with just the Bucket object in the past, hence avoided it out of practice :)

Line:163, scripts/thanosied.py -> Will do this in the next patch

Line:189, scripts/thanosied.py -> Will do this in the next patch

Line:245, scripts/thanosied.py -> We handle Timeouts and retry indefinetly as of now. Will try to include a limit to number of retries we do in the next patch.

Line:260, scripts/thanosied.py -> This is incase we try to have replicate_to as 1 and the bucket doesn't have replicas enable. We handle the error and retry.

Line:297, scripts/thanosied.py -> This is so that it can be used in the threadpool. We can have just a single method without the wrapper, but for better readability, I created the wrapper. If required, we can move to *args, **kargs model too.

Line:380, scripts/thanosied.py -> This is threadpool and not process pool. Note that we import from multiprocessing.dummy which is a Thread implementation and not MultiProcess implentation of Pool.

Line:382, scripts/thanosied.py -> You are right that we do not have anyother task being submitted. Included this just to be sure.

Line:409, scripts/thanosied.py -> Will work on having a single function in the next patch

Line:421, scripts/thanosied.py -> These are arguments to be sent to the function call and not the argument to the class. Will change the name of the variable to avoid confusion.

Line:489, scripts/thanosied.py -> You are right, it does. Will remove all unnecessary converts in the next patch.

Line:492, scripts/thanosied.py -> You are right, it does. Will remove all unnecessary converts in the next patch.

Line:493, scripts/thanosied.py -> In my experimentations, I haven't seen any performance degradation up until number of workers are 25. From there onwards, there is performance degradation and also after certain point, we create too many file handlers for the program to handle. I set hard limit for this first iteration, which can be removed in future. Will try to add another parameter so that the user can set the number instead of us deciding for the user.Â 

Line:498, scripts/thanosied.py -> Will fix in the next patch.

Line:500, scripts/thanosied.py -> My current logic is to send different argument for each of the worker (start and number of may vary for each worker). Hence we need to create copies of arguments without destroying the original arguments. Hence doing the deepcopy.

Line:531, scripts/thanosied.py -> I agree that it adds a bit of unwanted complexity by having both threads and processes. But in my experimentation this past week, I found that having both is necessary to obtain the high throughput that we needed for testing (I have seen around 200K ops/sec on normal boxes and about 800K+ ops/sec on the perf boxes ) As you mentioned, having only threads gave only about 25K ops, irrespective of number of threads I created. Hence have included both multi-processing and threads in the script.

Line:531, scripts/thanosied.py -> Good point. Will add this in the next patch.

----------------------------------------------------------------------------------------------------------------------
Author: Pavel Paulau
Date: 2018-06-12 20:31:01.000000000
Message: 
Removed reviewer Pavel Paulau.
----------------------------------------------------------------------------------------------------------------------
