======================================================================================================================
DESCRIPTION:

MB-35328 Handling KV failover from indexer

When a node is failed over, it can take arbitrarily long time for
ns_server on the failed over node to realise the failover and act
on it. When it is a KV node that is failed over, in some cases
(E.g. indexer is ahead of replica and ns_server takes more than
20 minutes to realise the failover), this can lead to a rollback to
"0" for indexer

The rollback situation can be avoided to some extent by making indexer
to act pro-actively on KV node failover rather than waiting for ns_server
of the failed over node to terminate the projector process/ intimate
memcached about failover

Change-Id: I4602f95f205d3a483c9bbdd550f5ffcb67c37a6c

======================================================================================================================
COMMENTS
======================================================================================================================
Author: VarunVelamuri
Date: 2019-09-24 14:15:53.000000000
Message: 
Uploaded patch set 5.
----------------------------------------------------------------------------------------------------------------------
Author: Deepkaran Salooja
Date: 2019-09-25 22:07:33.000000000
Message: 
Patch Set 5:

(1 comment)
Line:7901, secondary/indexer/indexer.go -> I don't think this method can detect a failover reliably e.g. if the method restarts and the node fails over and moves out of the cluster before this method is ready to receive the notification, it can miss a failover event. Instead of trying to detect a failover, a more robust mechanism would be to periodically compare the VBMap in TK with the cluster VBMap/list of active nodes to check if there is an extra node in our bookkeeping which needs to be removed.

----------------------------------------------------------------------------------------------------------------------
