======================================================================================================================
DESCRIPTION:

MB-51917:Limit to max 200 indexes per bucket

1. A new configurable setting introduced "indexer.settings.serverless.indexLimit", which indicates the maximum number of indexes that can be created per bucket.

2. This feature is only implemetd in Serverless mode.

3. The planner enforces this Limit so in serverless mode the CreateIndex uses planner mandatorily.

4. Fail Fast Mechanism is also implemeted for enforcing the limit.

Change-Id: I2a3de658b136606cfaa898aefa4e7750ca1770d3

======================================================================================================================
COMMENTS
======================================================================================================================
Author: singlaabhinandan
Date: 2022-07-21 11:19:54.000000000
Message: 
Patch Set 5: Patch Set 4 was rebased
----------------------------------------------------------------------------------------------------------------------
Author: VarunVelamuri
Date: 2022-07-26 06:04:24.000000000
Message: 
Patch Set 5:

(3 comments)
Line:432, secondary/manager/client/metadata_provider.go -> Will >= be a more elegant condition here?

Line:520, secondary/manager/client/metadata_provider.go -> If a bucket is created & deleted & created again - This number might give incorrect results. I believe, we should be comparing the bucket UUID as well

Line:1645, secondary/planner/executor.go -> Will >= be a more elegant condition here?

----------------------------------------------------------------------------------------------------------------------
Author: singlaabhinandan
Date: 2022-07-26 06:38:25.000000000
Message: 
Patch Set 5: Verified+1

(3 comments)
Line:432, secondary/manager/client/metadata_provider.go -> For eg: the current number of indexes is 9 and the limit is 10 and if we are trying to create a new index then in case of >= it will fail allowing at max only 9 indexes to be created.

Line:520, secondary/manager/client/metadata_provider.go -> We are traversing over the the list on indexes in this loop, so if a bucket is dropped the indexes are also dropped and the same is reflected in metadatarepo. And when we add a new bucket and create new indexes on it, a new indexDefnId is added to the list of indexes. So it is unlikely the deleted bucket will affect this loop. Verified the same with a few test cases.

Line:1645, secondary/planner/executor.go -> Same as above.

----------------------------------------------------------------------------------------------------------------------
Author: amithk
Date: 2022-07-28 05:59:18.000000000
Message: 
Patch Set 5:

(1 comment)
Line:330, secondary/queryport/client/settings.go -> Is the default value here 0? In that case, do you want to allow 0 indexes?

----------------------------------------------------------------------------------------------------------------------
Author: VarunVelamuri
Date: 2022-07-28 06:07:51.000000000
Message: 
Patch Set 5:

(3 comments)
Line:432, secondary/manager/client/metadata_provider.go -> yes. What I mean is, numIndexes >= serverlessIndexLimit is a more elegant condition rather than writing numIndexes + 1 > serverlessIndexLimit.

So, if numIndexes is 9 & we are going to create a new index, we will allow it. However, if numIndexes is 10, we will fail it.

In future, it might be difficult to understand why there was a "+1" to numIndexes if there is not enough context. Having >= will make it easily readable

Line:520, secondary/manager/client/metadata_provider.go -> Sure. The question is about how much time does it take for the repo to get updated with the over all deleted indexes. I think it might take some finite time.

Thinking overall about this topic, if there are few indexes in deleted state but still exist in repo, then we will fail the index creation until the repo is updated with out the index information. Same case with bucket deletion

I think we should count the number of non-deleted indexes instead of all the indexes so that we will be eventually consistent & DDL operations will not fail with num indexes limit. 

E.g., if customer is already using 200 indexes and we give this limit. They drop some indexes & try again. We say drop is successful but fail index creation as repo is not yet updated. This does not give a good user experience to customer. Retries always work but we should avoid retry as much as possible.

Line:1645, secondary/planner/executor.go -> Same as above

----------------------------------------------------------------------------------------------------------------------
Author: amithk
Date: 2022-07-28 06:14:23.000000000
Message: 
Patch Set 5:

(1 comment)
Line:520, secondary/manager/client/metadata_provider.go -> I believe all of these discussions fall under "Sync DDL" topic. :)

----------------------------------------------------------------------------------------------------------------------
Author: singlaabhinandan
Date: 2022-07-28 09:26:19.000000000
Message: 
Patch Set 5: Verified+1

(3 comments)
Line:432, secondary/manager/client/metadata_provider.go -> Yes, makes sense.

Line:520, secondary/manager/client/metadata_provider.go -> As discussed offline will create a new MB for this change.

Line:330, secondary/queryport/client/settings.go -> Using 200 incase the value is less than 0. Change added in new patch

----------------------------------------------------------------------------------------------------------------------
