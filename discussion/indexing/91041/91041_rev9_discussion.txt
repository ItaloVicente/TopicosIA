======================================================================================================================
DESCRIPTION:

MB-26307: moi - Throttle number of StoreToDisk by numCPU/4

When number of indexes > number of cores, the large number of
persistence go routines ends up causing CPU spikes which results in
increased query latency.
To solve this have a buffered channel = number of CPU cores and only
have as many persistence go routines as number of cores.
So the max number of go routines spawned = 4 * concurrency

Change-Id: If5c66d8f7ee86498fb1667f800d38f6679ada789

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Sundar Sridharan
Date: 2018-03-23 23:38:25.000000000
Message: 
Uploaded patch set 9.
----------------------------------------------------------------------------------------------------------------------
Author: Sarath Lakshman
Date: 2018-03-24 05:02:38.000000000
Message: 
Patch Set 9:

StoreToDisk callback looks good to me.

For dynamically changing the persistor threads may be we could do the following.
When initializing the channel buffer size, set it to a large number like 4xNumCPU (which is the max ever possible).


Now add tokens to the channel according to the configuration. Let's initially 8.

When UpdateConfig() is called, add or remove tokens from the channel according to the new setting. We should do it by launching a goroutine and hold the lock so that UpdateConfig() won't block.
----------------------------------------------------------------------------------------------------------------------
Author: Sarath Lakshman
Date: 2018-03-24 05:06:24.000000000
Message: 
Patch Set 9:

Since the concurrent tokens are global and not specific to memdb instance, it will be better to initialize and change them in indexer.handleConfigUpdate  just like memory quota.
----------------------------------------------------------------------------------------------------------------------
