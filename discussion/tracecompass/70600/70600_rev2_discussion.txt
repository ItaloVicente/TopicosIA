======================================================================================================================
DESCRIPTION:

ss: make node cache static

The state system is being used in many places now. A kernel trace
can have over 10 state systems running simultaneously. Therefore,
the caches have been taking more and more space. A summary reading
of heap dumps showed that for a kernel trace now, 512 mb is needed
for these caches. This will not scale.

This patch makes ONE cache for all of trace compass. There is a tiny
heuristic, each location is offsetted by the object's hashcode. This
is to avoid too much cache thrashing du to temporal co-locality.

The seek times in trace compass (going from one place to another)
are accelerated by a factor of 3 and are noticeable on an i7 with
16gb of ram and an ssd.

Moreover, this patch improves scalability, by making trace compass
not run out of memory after opening 5 kernel traces simultaneously.

Change-Id: I9c6509b0e32c04025fc66c0fdda3d31e971584b4
Signed-off-by: Matthew Khouzam <matthew.khouzam@ericsson.com>

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Matthew Khouzam
Date: 2016-04-14 14:36:22.000000000
Message: 
Uploaded patch set 2.
----------------------------------------------------------------------------------------------------------------------
Author: CI Bot
Date: 2016-04-14 14:55:36.000000000
Message: 
Patch Set 2:

Build Started https://hudson.eclipse.org/tracecompass/job/tracecompass-gerrit/7772/
----------------------------------------------------------------------------------------------------------------------
Author: Jonathan Rajotte Julien
Date: 2016-04-14 15:15:48.000000000
Message: 
Patch Set 2:

>The seek times in trace compass (going from one place to another)
>are accelerated by a factor of 3 and are noticeable on an i7 with
>16gb of ram and an ssd.

Was this with a benchmark present in tc source code ? If so could you  mention it with the result for before/after inside the commit msg.

Thanks
----------------------------------------------------------------------------------------------------------------------
Author: CI Bot
Date: 2016-04-14 15:35:07.000000000
Message: 
Patch Set 2: Code-Review+1

Build Successful 

https://hudson.eclipse.org/tracecompass/job/tracecompass-gerrit/7772/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2016-04-14 15:42:27.000000000
Message: 
Patch Set 2:

Unfortunately we don't have a benchmark yet. I instrumented trace compass. I know it's hard to reproduce.
----------------------------------------------------------------------------------------------------------------------
Author: Jonathan Rajotte Julien
Date: 2016-04-14 16:22:33.000000000
Message: 
Patch Set 2:

> Unfortunately we don't have a benchmark yet. I instrumented trace
 > compass. I know it's hard to reproduce.

Well in this case could you provide more information on your methodology and the actual number. The best you be to actually create a valid benchmark etc. but if we can have a good idea of how it was measured and it make sense it should be enough.
----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2016-04-14 18:00:11.000000000
Message: 
Patch Set 2:

Ok, first, open 7-8 kernel traces at the same time in trace compass before and after this patch, there should be a huge difference.

For the other one, I opened a kernel trace with many others, opened (5 if I recall) and trace compass had request tracing enabled. (debug config->tracing->tmf.core->request) then I saved the before and after traces searching at the same bookmarks. This is how I would benchmark it later.
----------------------------------------------------------------------------------------------------------------------
Author: Jonathan Rajotte Julien
Date: 2016-04-14 18:49:30.000000000
Message: 
Patch Set 2:

> Ok, first, open 7-8 kernel traces at the same time in trace compass
 > before and after this patch, there should be a huge difference.

So no actual number?

 > 
 > For the other one, I opened a kernel trace with many others, opened
 > (5 if I recall) and trace compass had request tracing enabled.
 > (debug config->tracing->tmf.core->request) then I saved the before
 > and after traces searching at the same bookmarks. This is how I
 > would benchmark it later.

Where there any real number here ? Query time ?

Where does the factor of 3 come from ? 

I do not doubt that this helps but numbers are numbers and should be present when a patch is solely related to performance.
----------------------------------------------------------------------------------------------------------------------
Author: Patrick Tasse
Date: 2016-04-14 18:56:25.000000000
Message: 
Patch Set 2:

I tested with 6 kernel analysis views opened, and opened 4 different kernel traces.

Without this patch my Eclipse slowed to molasses and crashed horribly 1 time

With this patch it crashed 0 times and was still very responsive ;)
----------------------------------------------------------------------------------------------------------------------
Author: Patrick Tasse
Date: 2016-04-14 19:04:34.000000000
Message: 
Patch Set 2:

I believe the performance improvement is mainly due to the decreased garbage collection? In that case the numbers I would put would be related to the memory usage e.g. total number of cached HT nodes.
----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2016-04-14 19:04:45.000000000
Message: 
Patch Set 2:

> No actual number

It depends on the size of traces and how much memory is allocated to tracecompass. Typically for 1gb of heap, you need 5 100mb kernel traces to hit this problem.

> Where there any real number here ? Query time ?

Not query time, but time from a click to the data being put to the screen.

> Where does the factor of 3 come from ? 

The queries dropped from 2.4 s to 800 ms in my tests, I was going to profile this more thoroughly. The problem is of course that trace compass is having issues juggling too many traces. This restores scalability with many kernel traces opened. I did not want to delay sharing the fix for such an issue due to the fact that I would have to come up with benchmarks before.

This should be exposed on istcfy when the benchmarks are done. I already have an idea, we should evaluate random seeks with a window of 100ms. and a binary search in the trace. You focus on a point zooming in progressively. But that is another patch. 

> I do not doubt that this helps but numbers are numbers and should be present when a patch is solely related to performance.
----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2016-04-14 19:08:28.000000000
Message: 
Patch Set 2:

Jonathan, thank you very much for these questions, they are very appreciated.
----------------------------------------------------------------------------------------------------------------------
Author: Alexandre Montplaisir
Date: 2016-04-14 19:27:29.000000000
Message: 
Patch Set 2:

(1 comment)

Nice improvement, I agree putting the cache static is a good approach. However the logic is getting a bit heavy at places, and we are still limited by the fact that the cache is very associative, which means we may end up wasting some space.

I'll try to cook up something with a Guava cache, I think it would fit well here too.
Line:160, statesystem/org.eclipse.tracecompass.statesystem.core/src/org/eclipse/tracecompass/internal/statesystem/core/backend/historytree/HT_IO.java -> Why do we forcibly load a node in the cache when we write it?

The cache is for reads only, it's not a read/write cache, so there shouldn't be any concept of write-through (which is more a "write-back" here).

----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2016-04-14 19:38:05.000000000
Message: 
Patch Set 2:

(1 comment)
Line:160, statesystem/org.eclipse.tracecompass.statesystem.core/src/org/eclipse/tracecompass/internal/statesystem/core/backend/historytree/HT_IO.java -> I don't know, I kept the logic as-is. I don't think it would pass review if I changed the cache policy AND the locality.

----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2016-04-14 19:38:38.000000000
Message: 
Patch Set 2:

@jonathan, the trace output looks like this btw 
[1460656674.932] [TID=058] [REQ] Req=10 STARTED
[1460656674.936] [TID=058] [REQ] Req=10 read first event
[1460656675.851] [TID=058] [REQ] Req=10 SUCCEEDED
[1460656675.851] [TID=058] [REQ] Req=10 COMPLETED (580228 events read)
----------------------------------------------------------------------------------------------------------------------
Author: Jonathan Rajotte Julien
Date: 2016-04-14 19:40:12.000000000
Message: 
Patch Set 2:

> @jonathan, the trace output looks like this btw
 > [1460656674.932] [TID=058] [REQ] Req=10 STARTED
 > [1460656674.936] [TID=058] [REQ] Req=10 read first event
 > [1460656675.851] [TID=058] [REQ] Req=10 SUCCEEDED
 > [1460656675.851] [TID=058] [REQ] Req=10 COMPLETED (580228 events
 > read)

Well get us some number in the commit msg !
:P
----------------------------------------------------------------------------------------------------------------------
Author: Jonathan Rajotte Julien
Date: 2016-04-14 19:42:00.000000000
Message: 
Patch Set 2:

> (1 comment)
 > 
 > Nice improvement, I agree putting the cache static is a good
 > approach. However the logic is getting a bit heavy at places, and
 > we are still limited by the fact that the cache is very
 > associative, which means we may end up wasting some space.
 > 
 > I'll try to cook up something with a Guava cache, I think it would
 > fit well here too.
@Alexmonthy
You mean for the current patch of as an improvement that can be applied after this patch ?
----------------------------------------------------------------------------------------------------------------------
Author: Alexandre Montplaisir
Date: 2016-04-14 22:26:38.000000000
Message: 
Patch Set 2:

> You mean for the current patch of as an improvement that can be applied after this patch ?

I meant as an alternative. What do you think of https://git.eclipse.org/r/#/c/70713/ ?
----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2016-04-15 00:59:14.000000000
Message: 
Patch Set 2:

This patch aims to minimally modify the code. 

> However the logic is getting a bit heavy at places.

This patch is 44 lines long, the alternative is 100.

Let's be clear, the two patches are performance patches, I would like to see performance impact of the other one too. I think that the other patch should be split into 3 patches:
 
1- replace cache with guava cache.
2- make cache static.
3- make cache write through.

And the impact should be observed on the three.

Thoughts? I will copy this comment on the other patch as it affects both.
----------------------------------------------------------------------------------------------------------------------
Author: Alexandre Montplaisir
Date: 2016-04-20 22:38:34.000000000
Message: 
Patch Set 2:

> This patch is 44 lines long, the alternative is 100.

LoC =/= complexity... it's quite often the opposite even!


But it would indeed be interesting to measure the impact of using a Guava cache vs a manual array-based one.

There are some figures in the commit messsage of this patch, but you could explain the procedure, which benchmarks etc, so that we can reproduce it? And then we can compare it with the Guava cache patch.
----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2016-04-21 03:35:30.000000000
Message: 
Patch Set 2:

Ok, first, do you know how to trace requests in trace compass? (launch with tracing, tmf.core->requests)

take a 100mb trace, I suggest Django-db, it seems normal.

seek 10-20 times before and after the patch, compare the times to service the seeks.

For the memory one, it is really straightforward. Open every trace in the test jar (excluding experiments test) and see how much memory is used. with 1gb of heap, you should have a bad time. 

Then apply the patch and try again.
----------------------------------------------------------------------------------------------------------------------
Author: Alexandre Montplaisir
Date: 2016-04-29 19:45:54.000000000
Message: 
Patch Set 2: Code-Review-1

(3 comments)

Ok we can go with this patch first, and eventually rebase the other one on top. This will allow easier testing and reverting if we find out it breaks ALL THE THINGS.

I'd like a better name than "Pair", otherwise it's gtg.
Line:39, statesystem/org.eclipse.tracecompass.statesystem.core/src/org/eclipse/tracecompass/internal/statesystem/core/backend/historytree/HT_IO.java -> Please don't call it "Pair" ;)

Something like "CacheElement" would be more meaningful.

Line:54, statesystem/org.eclipse.tracecompass.statesystem.core/src/org/eclipse/tracecompass/internal/statesystem/core/backend/historytree/HT_IO.java -> Ideally you would want hashCode/equals methods for this object. This would allow using .equals() directly in the checks below. But the way the checks are done now it works too.

Line:160, statesystem/org.eclipse.tracecompass.statesystem.core/src/org/eclipse/tracecompass/internal/statesystem/core/backend/historytree/HT_IO.java -> Ok.

Well thinking about it a bit it makes some sense, when we write a node we have it all ready in memory. Adding to the cache costs close to nothing, and would save a future read.

----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2016-04-30 00:39:10.000000000
Message: 
Patch Set 2:

(1 comment)

I will rename Pair to CachingElement, but please answer the questions about equals/hashcode. Those methods are useful for guava caches or even hashtable caches, but in an array, they are not. Why should we not keep them for a later patch? It only adds dead code to this one.
Line:54, statesystem/org.eclipse.tracecompass.statesystem.core/src/org/eclipse/tracecompass/internal/statesystem/core/backend/historytree/HT_IO.java -> I can add them, but they are useless as we only create the object if they match, adding the equals and cie here would change the flow, and I really am aiming for minimal intrusion.

----------------------------------------------------------------------------------------------------------------------
Author: Alexandre Montplaisir
Date: 2016-04-30 13:42:06.000000000
Message: 
Patch Set 2:

equals()/hashCode() was just a suggestion. Yeah you could keep it like that too, it works.

If you aim for minimal code you could also avoid the getters, and access the fields directly. Since it's a "struct" object it's acceptable.
----------------------------------------------------------------------------------------------------------------------
Author: Gerrit Code Review
Date: 2016-05-03 19:49:43.000000000
Message: 
Change has been successfully cherry-picked as 4018d70a5dc02af1f3c4d6a1d5724b341595bc72 by Matthew Khouzam
----------------------------------------------------------------------------------------------------------------------
