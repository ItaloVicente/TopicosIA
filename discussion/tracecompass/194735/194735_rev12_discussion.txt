======================================================================================================================
DESCRIPTION:

tmf.ui: Bisect row fetches in parallel recursively

Add the new RowModelsExecutor class, a delegate of
BaseDataProviderTimeGraphView meant for optionally fetching row models
in parallel. If the related system property is defined as a JVM
argument, then this class considers parallelism, depending on that
configurable threshold and current view zooming. The corresponding
javadoc explains the currently known values and their consequences.

Parallelize by bisecting row fetches if zoomed out enough, then. Execute
each bisected fetching of (sub) time ranges in parallel, while
collecting the resulting responses together yet consistently,
status-wise. Join the resulting models along with entry rows and states.

Base the RowModelsExecutor (a Callable) implementation on recursively
calling its contract call() method. The recursion terminates with the
actual call(s) to ITimeGraphDataProvider.fetchRowModel as the leaf,
based on its BaseDataProviderTimeGraphView.getFetchRowModelParameters.

Refactor BaseDataProviderTimeGraphView.zoomEntries so it delegates
getFetchRowModelParameters and fetchRowModel calls to the hereby added
RowModelsExecutor class. Introduce the fetchRowModel method in
BaseDataProviderTimeGraphView for this integration purpose.

Also add a getFetchRowModelParameters method to
BaseDataProviderTimeGraphView, which has a new 'delegate' boolean
parameter. The latter is for use by extensions of
BaseDataProviderTimeGraphView, for those that depend on currentThread
being a zoom thread or not, to behave accordingly. Document that
parameter in the method javadoc. Without this extension, subclasses end
up missing states while collecting them within the hereby added
RowModelsExecutor threads. The latter being zoom-thread delegates here,
thus to be treated as such down there.

Slightly amend TmfModelResponse to make it extensible; apply default
Eclipse formatting while there. Add the new TmfModelResponseMutable
class that extends TmfModelResponse this way.

Add the new 'NONE' ITmfResponse.Status enum value, for use in the
aforementioned TmfModelResponseMutable. Beside being a mutable
TmfModelResponse, the latter is meant as an updatable response that
collects post-NONE statuses as its model evolves. This includes an
optionally concurrent use, to safely collect status updates in parallel.

Fix the trivial yet default Eclipse formatting in CommonStatusMessage
while touching that file. Remove its trivial constructor javadoc. Also
add the missing newline in the companion messages.properties EOF while
there too. Do the corresponding amends in the related Messages class.

[Added] BaseDataProviderTimeGraphView.getFetchRowModelParameters w/ delegate
[Added] CommonStatusMessage.NONE
[Added] ITmfResponse.Status.NONE
[Added] Messages.CommonStatusMessage_None
[Added] TmfModelResponseMutable extending TmfModelResponse
[Added] o.e.t.tmf.ui.views.timegraph.RowModelsExecutor delegate class

[Changed] BaseDataProviderTimeGraphView.zoomEntries to use RowModelsExecutor
[Changed] TmfModelResponse which is now extensible

Change-Id: I439bb64d52cea0c3b32c61c20dae075e28a63bb8
Signed-off-by: Marco Miller <marco.miller@ericsson.com>

======================================================================================================================
COMMENTS
======================================================================================================================
Author: Marco Miller
Date: 2022-07-20 19:22:59.000000000
Message: 
Uploaded patch set 12: Patch Set 11 was rebased.
----------------------------------------------------------------------------------------------------------------------
Author: Marco Miller
Date: 2022-07-20 19:34:06.000000000
Message: 
Patch Set 12:

(1 comment)

This change is ready for review.
----------------------------------------------------------------------------------------------------------------------
Author: Trace Compass Bot
Date: 2022-07-20 20:05:58.000000000
Message: 
Patch Set 12:

Build Started https://ci.eclipse.org/tracecompass/job/tracecompass-gerrit-short-ui-only/2479/ (1/3)
----------------------------------------------------------------------------------------------------------------------
Author: Trace Compass Bot
Date: 2022-07-20 20:10:59.000000000
Message: 
Patch Set 12:

Build Started https://ci.eclipse.org/tracecompass/job/tracecompass-gerrit-core-only/2451/ (2/3)
----------------------------------------------------------------------------------------------------------------------
Author: Trace Compass Bot
Date: 2022-07-20 20:29:05.000000000
Message: 
Patch Set 12:

Build Started https://ci.eclipse.org/tracecompass/job/tracecompass-gerrit-long-ui-only/2506/ (3/3)
----------------------------------------------------------------------------------------------------------------------
Author: Trace Compass Bot
Date: 2022-07-20 21:11:07.000000000
Message: 
Patch Set 12: Verified+1

Build Successful 

https://ci.eclipse.org/tracecompass/job/tracecompass-gerrit-long-ui-only/2506/ : SUCCESS

https://ci.eclipse.org/tracecompass/job/tracecompass-gerrit-core-only/2451/ : SUCCESS

https://ci.eclipse.org/tracecompass/job/tracecompass-gerrit-short-ui-only/2479/ : SUCCESS
----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2022-07-21 01:26:02.000000000
Message: 
Patch Set 12:

(1 comment)
File Comment: /PATCHSET_LEVEL -> Quick non-review thing. [Added] and [Changed] is for the new and noteworthy, so it needs to be user-centric, not dev centric.
----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2022-07-21 02:21:00.000000000
Message: 
Patch Set 12: Code-Review-1

(1 comment)
File Comment: /PATCHSET_LEVEL -> I tried this. I traced it. I don't see the performance gain.

I think the reason is you have hard coded the values to a given trace size. So when I tried with my 3 week trace I get , and my 100 us MASSIVE trace, they both didn't work that well. :(

The three week trace has (OBVIOUSLY ;) ) an out of heap space... 

The 100 us trace was oh so slightly slower. Bare in mind, I tried it on a 1440p monitor that I rotated by 90 degrees so it has 2560 px on the row height, and zoomed out to 3 px / row.

Finally, the Goldilocks aka just right trace (1 second, 4000 entries) seems to have about the same performance on my machine with higher CPU usage.

I would argue that recursion is not the way to go. 

Simply 
1- make one executor with the number of CPUs you have
2- partition the request by the number of executors. e.g. 8 threads, 8 requests.

The solution you are proposing is at the same time more elegant, but will be less predictable in my opinion.
----------------------------------------------------------------------------------------------------------------------
Author: Marco Miller
Date: 2022-07-21 14:15:35.000000000
Message: 
Patch Set 12:

(2 comments)
File Comment: /PATCHSET_LEVEL -> > I tried this. I traced it. I don't see the performance gain.
> 
> I think the reason is you have hard coded the values to a given trace size. So when I tried with my 3 week trace I get , and my 100 us MASSIVE trace, they both didn't work that well. :(

I didn't hard-code any values. The value is configurable through that new system property. It was meant as to be tuned or tried empirically. It is totally possible and likely that it didn't perform (or, counter-performed), depending on the value(s) you tried; which ones did you define and why? -Just so I understand your experience better.

I myself have been testing this with that internal change's use case you were added to (as reviewer). Maybe I forgot to apply similar patches elsewhere in TC; will check.

> The three week trace has (OBVIOUSLY ;) ) an out of heap space... 
> 
> The 100 us trace was oh so slightly slower. Bare in mind, I tried it on a 1440p monitor that I rotated by 90 degrees so it has 2560 px on the row height, and zoomed out to 3 px / row.
> 
> Finally, the Goldilocks aka just right trace (1 second, 4000 entries) seems to have about the same performance on my machine with higher CPU usage.
> 
> I would argue that recursion is not the way to go. 
> 
> Simply 
> 1- make one executor with the number of CPUs you have
> 2- partition the request by the number of executors. e.g. 8 threads, 8 requests.
> 
> The solution you are proposing is at the same time more elegant, but will be less predictable in my opinion.

Could be, yes, over a diverse set of test cases. I'll work on this non-recursive alternative in parallel ;) so we can compare and decide. Please still see my requests for you further above. And thanks for the reviews and trials!
File Comment: /PATCHSET_LEVEL -> Please mark your addressable comments as un-Resolved, so we see them as to-do, Gerrit-wise; thanks. Now, the last times I omitted [Added|Changed|&c] footers in commit messages, I was rightfully asked to add them in. Here, I included the ones I thought were applicable to N&N readers. Please be more specific about which lines you think shouldn't be listed.
----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2022-07-21 14:20:04.000000000
Message: 
Patch Set 12:

(1 comment)
Line:61, /COMMIT_MSG -> I would add [Changed] request time graph rows in parallel when applicable.

This would make the n&n more digestible for our readers.

----------------------------------------------------------------------------------------------------------------------
Author: Marco Miller
Date: 2022-07-21 14:53:44.000000000
Message: 
Patch Set 12:

(2 comments)
Line:61, /COMMIT_MSG -> Ack, will think about this. I assume that your comment is not proposing the removal of these previous N&N lines though, above and below. Let me know otherwise.

File Comment: /PATCHSET_LEVEL -> Done, through the other related (new) comment thread.
----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2022-07-21 15:14:59.000000000
Message: 
Patch Set 12:

(1 comment)
Line:61, /COMMIT_MSG -> No, we just need something for the user to understand what's going on. Consider these tags to be TL;DRs for busy users.

----------------------------------------------------------------------------------------------------------------------
Author: Marco Miller
Date: 2022-07-21 15:25:23.000000000
Message: 
Patch Set 12:

(1 comment)
Line:61, /COMMIT_MSG -> Ack

----------------------------------------------------------------------------------------------------------------------
Author: Matthew Khouzam
Date: 2022-07-21 19:55:23.000000000
Message: 
Patch Set 12:

(2 comments)
File Comment: /PATCHSET_LEVEL -> Just a little helper for performance profiling.
File Comment: /PATCHSET_LEVEL -> One way to test it:

While tracing trace compass:
* Open a view at a fixed resolution.
* Have 5-6 bookmarked areas. Select each one, and press `Z`. This zooms to that area. 
* Trace the updating of the views with these ares.
* Apply your patch (or remove if it was already applied) 
* repeat
* import both traces into trace compass
* compare.
----------------------------------------------------------------------------------------------------------------------
Author: Marco Miller
Date: 2022-07-21 21:04:34.000000000
MISMATCHED INLINE COMMENT
Line:61, /COMMIT_MSG -> Done. I added a TL;DR as the first [Changed] line listed in the commit message. I also reordered these N&N lines overall, and amended for a few.
----------------------------------------------------------------------------------------------------------------------
Author: Marco Miller
Date: 2022-07-21 21:04:34.000000000
MISMATCHED INLINE COMMENT
File Comment: /PATCHSET_LEVEL -> Thanks. I replaced recursion with an availableProcessors-based implementation, which should be more predictable thus usable. I included a user-configurable switch to enable that parallelism (or not). If disabled, then only 1 such thread gets used by that implementation.
----------------------------------------------------------------------------------------------------------------------
